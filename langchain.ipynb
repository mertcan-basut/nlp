{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "RZtGHoHwraEl",
        "DIGo1r1jsk49",
        "alVvbSct2NvI",
        "LtBAoozGKFhW",
        "YY18mpsbDON-",
        "OI2ptmkKorF3",
        "Hb2fvIvhQFbG",
        "KKTMrVQHkFbX",
        "9-H3SawHJbxP"
      ],
      "authorship_tag": "ABX9TyMpfp1uuNXcwm1uN/HDKjqD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mertcan-basut/nlp/blob/main/langchain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q openai\n",
        "\n",
        "!pip install -q langchain langchain-openai langchain-experimental langchainhub\n",
        "!pip install -q docarray\n",
        "!pip install chromadb==0.4.14 # downgrade for sqlite3.OperationalError: attempt to write a readonly database\n",
        "!pip install -q pydantic==1.10.9 # downgrade for pydantic and Langchain compatibility: https://python.langchain.com/docs/guides/pydantic_compatibility\n",
        "!pip install -q wikipedia\n",
        "!pip install -q pypdf\n",
        "!pip install -q youtube-transcript-api pytube\n",
        "!pip install -q lark\n",
        "\n",
        "!pip install -q python-dotenv"
      ],
      "metadata": {
        "id": "wGmNpMwOKna1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!echo \"OPENAI_API_KEY=editme\" > .env"
      ],
      "metadata": {
        "id": "kZUgLx7JHh6Z"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "CjpO4-lVFA2z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "238dd4bb-1231-4d11-d1fe-d857c4c08f1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# a framework for developing LM powered applications\n",
        "from langchain_openai import ChatOpenAI # models\n",
        "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, SystemMessagePromptTemplate, PromptTemplate # input prompts\n",
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "from langchain.output_parsers import StructuredOutputParser # output parsers\n",
        "from langchain.output_parsers import ResponseSchema\n",
        "from langchain.chains import ConversationChain # memory\n",
        "from langchain.memory import ConversationBufferMemory, ConversationBufferWindowMemory, ConversationTokenBufferMemory, ConversationSummaryBufferMemory\n",
        "from langchain.chains import LLMChain, SimpleSequentialChain, SequentialChain # chains\n",
        "from langchain.chains.router import MultiPromptChain\n",
        "from langchain.chains.router.llm_router import LLMRouterChain, RouterOutputParser\n",
        "from langchain.chains import RetrievalQA # question answering\n",
        "from langchain.document_loaders import CSVLoader, PyPDFLoader, WebBaseLoader # document loaders\n",
        "from langchain_community.document_loaders.youtube import YoutubeLoader\n",
        "from langchain_community.document_loaders.text import TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter, TokenTextSplitter, MarkdownHeaderTextSplitter # splitting\n",
        "from langchain.vectorstores import DocArrayInMemorySearch, Chroma # vector stores\n",
        "from langchain.indexes import VectorstoreIndexCreator # indexes\n",
        "from langchain.embeddings import OpenAIEmbeddings # embeddings\n",
        "from langchain_community.llms.openai import OpenAI\n",
        "from langchain.retrievers.self_query.base import SelfQueryRetriever # retrievers\n",
        "from langchain.chains.query_constructor.schema import AttributeInfo\n",
        "from langchain.retrievers import ContextualCompressionRetriever\n",
        "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
        "from langchain.retrievers import SVMRetriever, TFIDFRetriever\n",
        "from langchain.evaluation.qa import QAEvalChain, QAGenerateChain # evaluation\n",
        "import langchain # debugging\n",
        "from langchain.agents import load_tools, AgentExecutor, create_react_agent, create_openai_functions_agent, tool # agents\n",
        "from langchain import hub\n",
        "from langchain_experimental.tools import PythonREPLTool\n",
        "\n",
        "import openai # direct API calls to OpenAI\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import json\n",
        "import os\n",
        "from datetime import date\n",
        "\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "_ = load_dotenv(find_dotenv()) # read local .env file\n",
        "# https://platform.openai.com/api-keys\n",
        "\n",
        "from IPython.display import display, Markdown\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "# https://s172-31-11-251p14136.lab-aws-production.deeplearning.ai/edit/OutdoorClothingCatalog_1000.csv\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "# /usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Models, Prompts, Parsers"
      ],
      "metadata": {
        "id": "RZtGHoHwraEl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"\\\n",
        "This leaf blower is pretty amazing.  It has four settings: \\\n",
        "candle blower, gentle breeze, windy city, and tornado. \\\n",
        "It arrived in two days, just in time for my wife's \\\n",
        "anniversary present. \\\n",
        "I think my wife liked it so much she was speechless. \\\n",
        "So far I've been the only one using it, and I've been \\\n",
        "using it every other morning to clear the leaves on our lawn. \\\n",
        "It's slightly more expensive than the other leaf blowers \\\n",
        "out there, but I think it's worth it for the extra features.\\\n",
        "\"\"\"\n",
        "\n",
        "system_message_template = \"\"\"\\\n",
        "For the following text, extract the following information:\n",
        "\n",
        "gift: Was the item purchased as a gift for someone else? Answer True if yes, False if not or unknown.\n",
        "\n",
        "delivery_days: How many days did it take for the product to arrive? If this information is not found, output -1.\n",
        "\n",
        "price_value: Extract any sentences about the value or price, and output them as a comma separated Python list.\n",
        "\"\"\"\n",
        "\n",
        "human_message_template = \"\"\"\\\n",
        "text: {text}\n",
        "\"\"\"\n",
        "\n",
        "format_instructions_template = \"\"\"\\\n",
        "Format the output as JSON with the following keys:\n",
        "gift\n",
        "delivery_days\n",
        "price_value\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "CuOOgUWy00AI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client = openai.OpenAI(api_key=os.environ['OPENAI_API_KEY'])\n",
        "\n",
        "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        temperature=0,\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "get_completion(\"Hi!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "ez7AjgGLJ7jq",
        "outputId": "00398d69-ff06-48f4-845e-66a396d9113b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hello! How can I assist you today?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def format_prompt(system_message, human_message, format_instructions):\n",
        "  return f\"\"\"\\\n",
        "{system_message}\n",
        "\n",
        "{human_message}\n",
        "\n",
        "{format_instructions}\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "system_message_prompt = system_message_template.format()\n",
        "human_message_prompt = human_message_template.format(text=text)\n",
        "format_instructions_prompt = format_instructions_template.format()\n",
        "prompt = format_prompt(system_message_prompt, human_message_prompt, format_instructions_prompt)\n",
        "print(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "srwP4Vv99JSI",
        "outputId": "ef3c3065-bde7-464f-9ac3-faa85cefee16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For the following text, extract the following information:\n",
            "\n",
            "gift: Was the item purchased as a gift for someone else? Answer True if yes, False if not or unknown.\n",
            "\n",
            "delivery_days: How many days did it take for the product to arrive? If this information is not found, output -1.\n",
            "\n",
            "price_value: Extract any sentences about the value or price, and output them as a comma separated Python list.\n",
            "\n",
            "\n",
            "text: This leaf blower is pretty amazing.  It has four settings: candle blower, gentle breeze, windy city, and tornado. It arrived in two days, just in time for my wife's anniversary present. I think my wife liked it so much she was speechless. So far I've been the only one using it, and I've been using it every other morning to clear the leaves on our lawn. It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\n",
            "\n",
            "\n",
            "Format the output as JSON with the following keys:\n",
            "gift\n",
            "delivery_days\n",
            "price_value\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = get_completion(prompt)\n",
        "print(response)\n",
        "\n",
        "output_dict = json.loads(response)\n",
        "output_dict.get('delivery_days')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YlXK5Ub0BNOD",
        "outputId": "9e7dc2ef-6603-45a6-fd90-de7de3897ed0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"gift\": true,\n",
            "  \"delivery_days\": 2,\n",
            "  \"price_value\": [\"It's slightly more expensive than the other leaf blowers out there\"]\n",
            "}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 136
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.0)\n",
        "\n",
        "chat([HumanMessage(content=\"Hi!\")]).content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "qlSD8PcbwgSN",
        "outputId": "a73b6afd-0cc9-457e-e3f1-39506162dc02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hello! How can I assist you today?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "system_message_prompt = SystemMessagePromptTemplate.from_template(system_message_template)\n",
        "human_message_prompt = HumanMessagePromptTemplate.from_template(human_message_template)\n",
        "\n",
        "gift_schema = ResponseSchema(name=\"gift\", description=\"Was the item purchased as a gift for someone else? Answer True if yes, False if not or unknown.\")\n",
        "delivery_days_schema = ResponseSchema(name=\"delivery_days\", description=\"How many days did it take for the product to arrive? If this information is not found, output -1.\")\n",
        "price_value_schema = ResponseSchema(name=\"price_value\", description=\"Extract any sentences about the value or price, and output them as a comma separated Python list.\")\n",
        "response_schemas = [gift_schema, delivery_days_schema, price_value_schema]\n",
        "\n",
        "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
        "\n",
        "format_instructions = output_parser.get_format_instructions()\n",
        "format_instructions_prompt = SystemMessage(content=format_instructions)\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt, format_instructions_prompt])\n",
        "prompt = prompt_template.format_messages(text=text)\n",
        "for message in prompt: print(message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-m-vJjDA5ZA",
        "outputId": "aa155e13-0b14-4294-aa19-7c788e054f82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For the following text, extract the following information:\n",
            "\n",
            "gift: Was the item purchased as a gift for someone else? Answer True if yes, False if not or unknown.\n",
            "\n",
            "delivery_days: How many days did it take for the product to arrive? If this information is not found, output -1.\n",
            "\n",
            "price_value: Extract any sentences about the value or price, and output them as a comma separated Python list.\n",
            "\n",
            "text: This leaf blower is pretty amazing.  It has four settings: candle blower, gentle breeze, windy city, and tornado. It arrived in two days, just in time for my wife's anniversary present. I think my wife liked it so much she was speechless. So far I've been the only one using it, and I've been using it every other morning to clear the leaves on our lawn. It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\n",
            "\n",
            "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
            "\n",
            "```json\n",
            "{\n",
            "\t\"gift\": string  // Was the item purchased as a gift for someone else? Answer True if yes, False if not or unknown.\n",
            "\t\"delivery_days\": string  // How many days did it take for the product to arrive? If this information is not found, output -1.\n",
            "\t\"price_value\": string  // Extract any sentences about the value or price, and output them as a comma separated Python list.\n",
            "}\n",
            "```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = chat(prompt).content\n",
        "print(response)\n",
        "\n",
        "output_dict = output_parser.parse(response)\n",
        "output_dict.get('delivery_days')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3gB1eU40xo2",
        "outputId": "6c69f3cd-fd26-4871-9a99-f01b35a2ad7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "```json\n",
            "{\n",
            "\t\"gift\": true,\n",
            "\t\"delivery_days\": 2,\n",
            "\t\"price_value\": \"It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\"\n",
            "}\n",
            "```\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Memory"
      ],
      "metadata": {
        "id": "DIGo1r1jsk49"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(temperature=0.0, model=\"gpt-3.5-turbo\")\n",
        "memory = ConversationBufferMemory()\n",
        "conversation = ConversationChain(llm=llm, memory=memory, verbose=True)"
      ],
      "metadata": {
        "id": "OwVMxqjmsmnm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(conversation.predict(input=\"Hi, my name is Mert.\"))\n",
        "print(conversation.predict(input=\"What is 1+1?\"))\n",
        "print(conversation.predict(input=\"What is my name?\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MWPx4j9UtNcL",
        "outputId": "29383f61-b9bd-4c75-9423-a9ac1363a38c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: Hi, my name is Mert.\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Hello Mert! It's nice to meet you. How can I assist you today?\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Hi, my name is Mert.\n",
            "AI: Hello Mert! It's nice to meet you. How can I assist you today?\n",
            "Human: What is 1+1?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "1+1 equals 2. Is there anything else you would like to know?\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Hi, my name is Mert.\n",
            "AI: Hello Mert! It's nice to meet you. How can I assist you today?\n",
            "Human: What is 1+1?\n",
            "AI: 1+1 equals 2. Is there anything else you would like to know?\n",
            "Human: What is my name?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Your name is Mert. Is there anything else you would like to know or discuss?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(memory.buffer, '\\n')\n",
        "memory.load_memory_variables({})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZybXyJK4t6Yl",
        "outputId": "e9de7485-0da6-4733-b583-2da42c9e7095"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Human: Hi, my name is Mert.\n",
            "AI: Hello Mert! It's nice to meet you. How can I assist you today?\n",
            "Human: What is 1+1?\n",
            "AI: 1 + 1 equals 2. Is there anything else you would like to know?\n",
            "Human: What is my name?\n",
            "AI: Your name is Mert. Is there anything else you would like to know or discuss? \n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': \"Human: Hi, my name is Mert.\\nAI: Hello Mert! It's nice to meet you. How can I assist you today?\\nHuman: What is 1+1?\\nAI: 1 + 1 equals 2. Is there anything else you would like to know?\\nHuman: What is my name?\\nAI: Your name is Mert. Is there anything else you would like to know or discuss?\"}"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# add aditional data to memory\n",
        "memory.save_context({\"input\": \"Hi!\"}, {\"output\": \"What's up?\"})\n",
        "print(memory.buffer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uTgquHyKukqJ",
        "outputId": "522db347-d635-4ee9-d703-e03a3e716e1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Human: Hi, my name is Mert.\n",
            "AI: Hello Mert! It's nice to meet you. How can I assist you today?\n",
            "Human: What is 1+1?\n",
            "AI: 1 + 1 equals 2. Is there anything else you would like to know?\n",
            "Human: What is my name?\n",
            "AI: Your name is Mert. Is there anything else you would like to know or discuss?\n",
            "Human: Hi!\n",
            "AI: What's up?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "memory = ConversationBufferWindowMemory(k=1) # only keeps most recent `k` conversations\n",
        "\n",
        "memory.save_context({\"input\": \"Hi!\"}, {\"output\": \"What's up?\"})\n",
        "memory.save_context({\"input\": \"Not much, just hanging.\"}, {\"output\": \"Cool.\"})\n",
        "\n",
        "print(memory.buffer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gOdw6gLHv8Nf",
        "outputId": "a672f9aa-7de3-45d2-8273-21feeda5f982"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Human: Not much, just hanging.\n",
            "AI: Cool.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=30) # chops off the earlier parts of the conversation to not exceed the token limit dependent on the LLM because usually cost is determined by number of tokens\n",
        "\n",
        "memory.save_context({\"input\": \"AI is what?!\"}, {\"output\": \"Amazing!\"})\n",
        "memory.save_context({\"input\": \"Backpropagation is what?\"}, {\"output\": \"Beautiful!\"})\n",
        "memory.save_context({\"input\": \"Chatbots are what?\"}, {\"output\": \"Charming!\"})\n",
        "\n",
        "print(memory.buffer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OZJ_TWC_w8v3",
        "outputId": "67c8c412-abca-4aa6-9242-36fe2e280fac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AI: Beautiful!\n",
            "Human: Chatbots are what?\n",
            "AI: Charming!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=30) # uses the specified LLM to summarize the entire chat history which exceeds the specified number of tokens\n",
        "\n",
        "memory.save_context({\"input\": \"AI is what?!\"}, {\"output\": \"Amazing!\"})\n",
        "memory.save_context({\"input\": \"Backpropagation is what?\"}, {\"output\": \"Beautiful!\"})\n",
        "memory.save_context({\"input\": \"Chatbots are what?\"}, {\"output\": \"Charming!\"})\n",
        "\n",
        "print(memory.load_memory_variables({})['history'])\n",
        "# `System` is not official OpenAI system message!"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aspHswv2x1IW",
        "outputId": "4a77020d-74bb-44f9-f34d-d971dfa27620"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "System: The human expresses surprise at the AI's positive view of artificial intelligence. The AI responds with \"Amazing!\" and the human asks about backpropagation.\n",
            "AI: Beautiful!\n",
            "Human: Chatbots are what?\n",
            "AI: Charming!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chains"
      ],
      "metadata": {
        "id": "alVvbSct2NvI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(temperature=0.9, model=\"gpt-3.5-turbo\")\n",
        "\n",
        "product = \"Queen Size Sheet Set\"\n",
        "review = \"Je trouve le goût médiocre. La mousse ne tient pas, c'est bizarre. J'achète les mêmes dans le commerce et le goût est bien meilleur...\\nVieux lot ou contrefaçon !?\""
      ],
      "metadata": {
        "id": "8plS0FvW5lnP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = ChatPromptTemplate.from_template(\"What is the best name to describe a company that makes {product}?\")\n",
        "chain = LLMChain(llm=llm, prompt=prompt_template) # LLM + prompt (most basic)\n",
        "\n",
        "chain.run(product)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "QhHkL91v2PCG",
        "outputId": "e1477dda-1c57-4e0d-916e-de3870048e55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Royal Comfort Bedding'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ideal when expecting 1 input & returning 1 output for every chain in sequence (output of the previous chain passed as input into the next chain)\n",
        "prompt_template_1 = ChatPromptTemplate.from_template(\"What is the best name to describe a company that makes {product}?\")\n",
        "chain_1 = LLMChain(llm=llm, prompt=prompt_template_1)\n",
        "\n",
        "prompt_template_2 = ChatPromptTemplate.from_template(\"Write a 20 words description for the following company:{company_name}\")\n",
        "chain_2 = LLMChain(llm=llm, prompt=prompt_template_2)\n",
        "\n",
        "chain = SimpleSequentialChain(chains=[chain_1, chain_2], verbose=True)\n",
        "\n",
        "chain.run(product)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "id": "HIB9ooML5JeZ",
        "outputId": "e44f2ace-3142-4fd7-c32f-08a5cf658fd5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
            "\u001b[36;1m\u001b[1;3mRegal Dreams Co.\u001b[0m\n",
            "\u001b[33;1m\u001b[1;3mRegal Dreams Co. specializes in luxury home decor items, offering elegant and timeless pieces to elevate any living space.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Regal Dreams Co. specializes in luxury home decor items, offering elegant and timeless pieces to elevate any living space.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sequentially executed multiple inputs & multiple outputs (more complicated)\n",
        "prompt_template_1 = ChatPromptTemplate.from_template(\"Translate the following review to english:\\n\\n{Review}\")\n",
        "chain_1 = LLMChain(llm=llm, prompt=prompt_template_1, output_key=\"English_Review\")\n",
        "\n",
        "prompt_template_2 = ChatPromptTemplate.from_template(\"Can you summarize the following review in 1 sentence:\\n\\n{English_Review}\")\n",
        "chain_2 = LLMChain(llm=llm, prompt=prompt_template_2, output_key=\"summary\")\n",
        "\n",
        "prompt_template_3 = ChatPromptTemplate.from_template(\"What language is the following review:\\n\\n{Review}\")\n",
        "chain_3 = LLMChain(llm=llm, prompt=prompt_template_3, output_key=\"language\")\n",
        "\n",
        "prompt_template_4 = ChatPromptTemplate.from_template(\n",
        "    \"Write a follow up response to the following \"\n",
        "    \"summary in the specified language:\"\n",
        "    \"\\n\\nSummary: {summary}\\n\\nLanguage: {language}\"\n",
        ")\n",
        "chain_4 = LLMChain(llm=llm, prompt=prompt_template_4, output_key=\"followup_message\")\n",
        "\n",
        "chain = SequentialChain(\n",
        "    chains=[chain_1, chain_2, chain_3, chain_4],\n",
        "    input_variables=[\"Review\"],\n",
        "    output_variables=[\"English_Review\", \"summary\", \"followup_message\"],\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "chain(review)\n",
        "# `run` not supported when there is not exactly one output key"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pW-zdnRD7fjp",
        "outputId": "c5123158-7a9d-4c2b-9c4f-50fc8261bca5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Review': \"Je trouve le goût médiocre. La mousse ne tient pas, c'est bizarre. J'achète les mêmes dans le commerce et le goût est bien meilleur...\\nVieux lot ou contrefaçon !?\",\n",
              " 'English_Review': \"I find the taste mediocre. The foam doesn't hold, it's weird. I buy the same ones in stores and the taste is much better... Old batch or counterfeit!?\",\n",
              " 'summary': 'The reviewer is dissatisfied with the taste and foam of the product, suspecting it may be an old batch or counterfeit.',\n",
              " 'followup_message': \"Je vous remercie pour votre avis sur notre produit. Nous sommes désolés que vous ayez été insatisfait de son goût et de sa mousse. Nous prenons très au sérieux la qualité de nos produits et nous aimerions en savoir plus sur votre expérience pour investiguer davantage. S'il vous plaît, contactez notre service clientèle pour que nous puissions résoudre ce problème au plus vite. Merci de nous avoir informés.\"}"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "physics_template = \"\"\"You are a very smart physics professor. \\\n",
        "You are great at answering questions about physics in a concise \\\n",
        "and easy to understand manner. \\\n",
        "When you don't know the answer to a question you admit \\\n",
        "that you don't know.\n",
        "\n",
        "Here is a question:\n",
        "{input}\"\"\"\n",
        "\n",
        "math_template = \"\"\"You are a very good mathematician. \\\n",
        "You are great at answering math questions. \\\n",
        "You are so good because you are able to break down \\\n",
        "hard problems into their component parts, \\\n",
        "answer the component parts, and then put them together\\\n",
        "to answer the broader question.\n",
        "\n",
        "Here is a question:\n",
        "{input}\"\"\"\n",
        "\n",
        "history_template = \"\"\"You are a very good historian. \\\n",
        "You have an excellent knowledge of and understanding of people, \\\n",
        "events and contexts from a range of historical periods. \\\n",
        "You have the ability to think, reflect, debate, discuss and \\\n",
        "evaluate the past. You have a respect for historical evidence \\\n",
        "and the ability to make use of it to support your explanations \\\n",
        "and judgements.\n",
        "\n",
        "Here is a question:\n",
        "{input}\"\"\"\n",
        "\n",
        "computerscience_template = \"\"\" You are a successful computer scientist. \\\n",
        "You have a passion for creativity, collaboration, \\\n",
        "forward-thinking, confidence, strong problem-solving capabilities, \\\n",
        "understanding of theories and algorithms, and excellent communication \\\n",
        "skills. You are great at answering coding questions. \\\n",
        "You are so good because you know how to solve a problem by \\\n",
        "describing the solution in imperative steps \\\n",
        "that a machine can easily interpret and you know how to \\\n",
        "choose a solution that has a good balance between \\\n",
        "time complexity and space complexity.\n",
        "\n",
        "Here is a question:\n",
        "{input}\"\"\"\n",
        "\n",
        "prompt_infos = [\n",
        "    {\n",
        "        \"name\": \"physics\",\n",
        "        \"description\": \"Good for answering questions about physics\",\n",
        "        \"prompt_template\": physics_template\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"math\",\n",
        "        \"description\": \"Good for answering math questions\",\n",
        "        \"prompt_template\": math_template\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"History\",\n",
        "        \"description\": \"Good for answering history questions\",\n",
        "        \"prompt_template\": history_template\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"computer science\",\n",
        "        \"description\": \"Good for answering computer science questions\",\n",
        "        \"prompt_template\": computerscience_template\n",
        "    }\n",
        "]\n",
        "\n",
        "# chains that will be called by router chain\n",
        "destination_chains = {}\n",
        "for p_info in prompt_infos:\n",
        "    name = p_info[\"name\"]\n",
        "    prompt_template = p_info[\"prompt_template\"]\n",
        "    prompt = ChatPromptTemplate.from_template(template=prompt_template)\n",
        "    chain = LLMChain(llm=llm, prompt=prompt)\n",
        "    destination_chains[name] = chain\n",
        "\n",
        "destinations = [f\"{p['name']}: {p['description']}\" for p in prompt_infos]\n",
        "destinations_str = \"\\n\".join(destinations)\n",
        "\n",
        "# called when the router can't decide which sub-chain to use\n",
        "default_prompt = ChatPromptTemplate.from_template(\"{input}\")\n",
        "default_chain = LLMChain(llm=llm, prompt=default_prompt)\n",
        "\n",
        "# template that will be used by LLM to route between chains\n",
        "MULTI_PROMPT_ROUTER_TEMPLATE = \"\"\"Given a raw text input to a \\\n",
        "language model select the model prompt best suited for the input. \\\n",
        "You will be given the names of the available prompts and a \\\n",
        "description of what the prompt is best suited for. \\\n",
        "You may also revise the original input if you think that revising \\\n",
        "it will ultimately lead to a better response from the language model.\n",
        "\n",
        "<< FORMATTING >>\n",
        "Return a markdown code snippet with a JSON object formatted to look like:\n",
        "```json\n",
        "{{{{\n",
        "    \"destination\": string \\\\ name of the prompt to use or \"DEFAULT\"\n",
        "    \"next_inputs\": string \\\\ a potentially modified version of the original input\n",
        "}}}}\n",
        "```\n",
        "\n",
        "REMEMBER: \"destination\" MUST be one of the candidate prompt \\\n",
        "names specified below OR it can be \"DEFAULT\" if the input is not \\\n",
        "well suited for any of the candidate prompts.\n",
        "REMEMBER: \"next_inputs\" can just be the original input \\\n",
        "if you don't think any modifications are needed.\n",
        "\n",
        "<< CANDIDATE PROMPTS >>\n",
        "{destinations}\n",
        "\n",
        "<< INPUT >>\n",
        "{{input}}\n",
        "\n",
        "<< OUTPUT (remember to include the ```json)>>\"\"\"\n",
        "\n",
        "router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(destinations=destinations_str)\n",
        "router_prompt = PromptTemplate(template=router_template, input_variables=[\"input\"], output_parser=RouterOutputParser()) # parses the LLM output to determine which chain to use and what the output to that chain should be\n",
        "\n",
        "# for routing between multiple prompt templates\n",
        "router_chain = LLMRouterChain.from_llm(llm, router_prompt)\n",
        "\n",
        "# decides on which sub-chain to route the response as an input (more complex) according to the prompt templates by passing the descriptions to an LLM\n",
        "chain = MultiPromptChain(router_chain=router_chain, destination_chains=destination_chains, default_chain=default_chain, verbose=True)\n",
        "\n",
        "chain.run(\"What is black body radiation?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "id": "NsWgl2isAb89",
        "outputId": "e74eb9eb-5336-417e-f174-7bfc4e4f1cfc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
            "physics: {'input': 'What is black body radiation?'}\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Black body radiation is the electromagnetic radiation emitted by a perfect black body, which absorbs all incoming radiation and emits energy at all wavelengths. The distribution of this radiation follows Planck's law, which describes how the intensity of the radiation changes with temperature. Black body radiation is an important concept in physics and has applications in various fields, including astrophysics and thermodynamics.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question and Answer"
      ],
      "metadata": {
        "id": "LtBAoozGKFhW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Please list all your shirts with sun protection in a table in markdown and summarize each one.\""
      ],
      "metadata": {
        "id": "llfVJDH0gQIh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the documents\n",
        "# because this documents are small, chunking (splitting documents into smaller pieces) isn't necessary\n",
        "loader = CSVLoader(file_path=\"./drive/MyDrive/dataset/OutdoorClothingCatalog_500.csv\", encoding='utf-8')\n",
        "\n",
        "docs = loader.load()\n",
        "docs[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gHweuizuLh10",
        "outputId": "be7bb34b-f5d1-46be-f1df-24858aa2a697"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content=\": 0\\nUnnamed: 0: 0\\nname: Women's Campside Oxfords\\ndescription: This ultracomfortable lace-to-toe Oxford boasts a super-soft canvas, thick cushioning, and quality construction for a broken-in feel from the first time you put them on. \\n\\nSize & Fit: Order regular shoe size. For half sizes not offered, order up to next whole size. \\n\\nSpecs: Approx. weight: 1 lb.1 oz. per pair. \\n\\nConstruction: Soft canvas material for a broken-in feel and look. Comfortable EVA innersole with Cleansport NXT® antimicrobial odor control. Vintage hunt, fish and camping motif on innersole. Moderate arch contour of innersole. EVA foam midsole for cushioning and support. Chain-tread-inspired molded rubber outsole with modified chain-tread pattern. Imported. \\n\\nQuestions? Please contact us for any inquiries.\", metadata={'source': './drive/MyDrive/dataset/OutdoorClothingCatalog_500.csv', 'row': 0})"
            ]
          },
          "metadata": {},
          "execution_count": 257
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# numerical representations of pieces of text that captures semantic meaning\n",
        "embeddings = OpenAIEmbeddings()\n",
        "\n",
        "embed = embeddings.embed_query(query)\n",
        "print(len(embed))\n",
        "embed[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QmKclZLpMDrf",
        "outputId": "a7ae9ae1-424c-46b3-8a0d-4e8f05598bba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1536\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.003285329438041333,\n",
              " 0.001319983088892754,\n",
              " 0.023928350673644135,\n",
              " -0.032684640238161373,\n",
              " -0.006897642983824973]"
            ]
          },
          "metadata": {},
          "execution_count": 258
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# basic vector store without needing to connect an external db\n",
        "db = DocArrayInMemorySearch.from_documents(docs, embeddings)\n",
        "\n",
        "# pieces of text with similar contents will have similar embeddings\n",
        "docs = db.similarity_search(query)\n",
        "print(len(docs))\n",
        "docs[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DAxTbHiBM7XU",
        "outputId": "717fe48a-b0d5-4c23-be4f-54993ce00ea2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content=\": 374\\nUnnamed: 0: 374\\nname: Men's Plaid Tropic Shirt, Short-Sleeve\\ndescription: Our Ultracomfortable sun protection is rated to UPF 50+, helping you stay cool and dry. Originally designed for fishing, this lightest hot-weather shirt offers UPF 50+ coverage and is great for extended travel. SunSmart technology blocks 98% of the sun's harmful UV rays, while the high-performance fabric is wrinkle-free and quickly evaporates perspiration. Made with 52% polyester and 48% nylon, this shirt is machine washable and dryable. Additional features include front and back cape venting, two front bellows pockets and an imported design. With UPF 50+ coverage, you can limit sun exposure and feel secure with the highest rated sun protection available.\", metadata={'source': './drive/MyDrive/dataset/OutdoorClothingCatalog_500.csv', 'row': 374})"
            ]
          },
          "metadata": {},
          "execution_count": 259
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm = OpenAI(temperature=0.0, model=\"gpt-3.5-turbo-instruct\")\n",
        "# create vector store\n",
        "index = VectorstoreIndexCreator(vectorstore_cls=DocArrayInMemorySearch, embedding=embeddings).from_loaders([loader])\n",
        "\n",
        "response = index.query(query, llm=llm)\n",
        "display(Markdown(response))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        },
        "id": "A-FINesa_IXH",
        "outputId": "722b15d8-bbec-453e-abec-c6446463a195"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n\n| Name | Description | Sun Protection Rating |\n| --- | --- | --- |\n| Men's Plaid Tropic Shirt, Short-Sleeve | Ultracomfortable sun protection rated to UPF 50+. Made with 52% polyester and 48% nylon. Features front and back cape venting, two front bellows pockets. | UPF 50+ |\n| Sun Shield Shirt by | High-performance sun shirt with UPF 50+ rating. Made with 78% nylon and 22% Lycra Xtra Life fiber. Wicks moisture and abrasion resistant. | UPF 50+ |\n| Girls' Ocean Breeze Long-Sleeve Stripe Shirt | Long-sleeve sun-protection rash guard with UPF 50+ rating. Made with Nylon Lycra®-elastane blend. Quick-drying and fade-resistant. | UPF 50+ |\n\nEach of these shirts offers sun protection with a UPF 50+ rating, blocking 98% of the sun's harmful UV rays. They are all made with high-performance fabrics that are quick-drying and recommended by The Skin Cancer Foundation. The Men's Plaid Tropic Shirt and Sun Shield Shirt also have additional features such as venting and pockets"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qdocs = \"\".join([docs[i].page_content for i in range(len(docs))])\n",
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
        "response = llm([HumanMessage(content=f\"{qdocs}\\nQuestion: {query}\")]).content\n",
        "display(Markdown(response))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "ld_yF2EnOe83",
        "outputId": "c4107ff1-b721-45ed-8ac9-ea4c891f8e4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "| Shirt Name                                      | Summary                                                                                                                                                                                                                                                                          |\n|-----------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Men's Plaid Tropic Shirt, Short-Sleeve        | This shirt offers UPF 50+ sun protection, blocks 98% of harmful UV rays, is wrinkle-free, and quickly evaporates perspiration. Made with 52% polyester and 48% nylon, it features front and back cape venting and two front bellows pockets.                    |\n| Sun Shield Shirt by                           | This high-performance sun shirt provides SPF 50+ sun protection, blocks 98% of harmful rays, and is made of 78% nylon and 22% Lycra Xtra Life fiber. It is quick-drying, abrasion-resistant, and fits comfortably over swimsuits.                                                |\n| Girls' Ocean Breeze Long-Sleeve Stripe Shirt | This long-sleeve rash guard offers full-coverage sun protection with UPF 50+. Made of Nylon Lycra-elastane blend, it is quick-drying, fade-resistant, and seawater-resistant. Recommended by The Skin Cancer Foundation for UV protection.                        |"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm = OpenAI(temperature=0.0, model=\"gpt-3.5-turbo-instruct\")\n",
        "retriever = db.as_retriever() # takes in a query and return fetched documents from the vector store\n",
        "\n",
        "qa_stuff = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type='stuff', # stuffs all the documents into context for a single call to the LLM (cheap but not ideal for large documents)\n",
        "    # other methods:\n",
        "    # -map_reduce: make multiple calls with each retrieved document, then make a final call to summarize the answers (popular even for summarization, supports large and vast amount of documents, supports parallel fast computing, but expensive and can't evaluate all of the information at once)\n",
        "    # -refine: iterativly make multiple calls with each retrieved document by building on the answer (combines information, but expensive and slow because each step depends on the previous one)\n",
        "    # -map_rerank: make multiple calls with each retrieved document and ask for a score from the LLM, then select the most relevant answer with the highest score (experimental, relies on the LLM to know the score, expensive)\n",
        "    retriever=retriever,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "response = qa_stuff.run(query)\n",
        "display(Markdown(response))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "id": "OgC4PV5tNkmx",
        "outputId": "b4122d7a-b0bf-4c75-b242-38cde706bfb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n\n| Name | Description | Sun Protection Rating |\n| --- | --- | --- |\n| Men's Plaid Tropic Shirt, Short-Sleeve | Made with UPF 50+ coverage, blocks 98% of harmful UV rays, wrinkle-free, quick-drying | UPF 50+ |\n| Sun Shield Shirt by | High-performance fabric with SPF 50+ sun protection, wicks moisture, abrasion resistant | SPF 50+ |\n| Girls' Ocean Breeze Long-Sleeve Stripe Shirt | Made with UPF 50+ coverage, blocks 98% of harmful UV rays, quick-drying, fade-resistant | UPF 50+ |\n| Classic Plaid Short-Sleeve Shirt | Made with pure European flax, lightweight and breathable | N/A |"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation"
      ],
      "metadata": {
        "id": "YY18mpsbDON-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loader = CSVLoader(file_path=\"./drive/MyDrive/dataset/OutdoorClothingCatalog_500.csv\")\n",
        "data = loader.load()\n",
        "\n",
        "index = VectorstoreIndexCreator(\n",
        "    vectorstore_cls=DocArrayInMemorySearch\n",
        ").from_loaders([loader])\n",
        "\n",
        "llm = ChatOpenAI(temperature = 0.0, model=\"gpt-3.5-turbo\")\n",
        "\n",
        "qa = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=index.vectorstore.as_retriever(),\n",
        "    verbose=True,\n",
        "    chain_type_kwargs = {\"document_separator\": \"<<<<>>>>>\"}\n",
        ")"
      ],
      "metadata": {
        "id": "ozSL6ReWDPlr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa_examples = [\n",
        "  {\n",
        "    \"query\": \"Do the Cozy Comfort Pullover Set have side pockets?\",\n",
        "    \"answer\": \"Yes\"\n",
        "  },\n",
        "  {\n",
        "    \"query\": \"What collection is the Ultra-Lofty 850 Stretch Down Hooded Jacket from?\",\n",
        "    \"answer\": \"The DownTek collection\"\n",
        "  }\n",
        "]\n",
        "\n",
        "qa_gen_chain = QAGenerateChain.from_llm(ChatOpenAI(model=\"gpt-3.5-turbo\"))\n",
        "\n",
        "# generate questions to the documents using an LLM\n",
        "new_examples = qa_gen_chain.apply_and_parse([{\"doc\": t} for t in data[:5]])\n",
        "new_examples = list(map(lambda outer_dict: outer_dict['qa_pairs'], new_examples)) # remove outer dictionaries ('qa_pairs') from the response schema\n",
        "qa_examples += new_examples\n",
        "\n",
        "print(data[4].page_content)\n",
        "qa_examples[-1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YItf4TJHISwr",
        "outputId": "941e5b01-6463-4e1a-d75e-27c092226682"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ": 4\n",
            "Unnamed: 0: 4\n",
            "name: EcoFlex 3L Storm Pants\n",
            "description: Our new TEK O2 technology makes our four-season waterproof pants even more breathable. It's guaranteed to keep you dry and comfortable – whatever the activity and whatever the weather. Size & Fit: Slightly Fitted through hip and thigh. \n",
            "\n",
            "Why We Love It: Our state-of-the-art TEK O2 technology offers the most breathability we've ever tested. Great as ski pants, they're ideal for a variety of outdoor activities year-round. Plus, they're loaded with features outdoor enthusiasts appreciate, including weather-blocking gaiters and handy side zips. Air In. Water Out. See how our air-permeable TEK O2 technology keeps you dry and comfortable. \n",
            "\n",
            "Fabric & Care: 100% nylon, exclusive of trim. Machine wash and dry. \n",
            "\n",
            "Additional Features: Three-layer shell delivers waterproof protection. Brand new TEK O2 technology provides enhanced breathability. Interior gaiters keep out rain and snow. Full side zips for easy on/off over boots. Two zippered hand pockets. Thigh pocket. Imported.\n",
            "\n",
            " – Official Supplier to the U.S. Ski Team\n",
            "THEIR WILL\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'query': 'What technology is featured in the EcoFlex 3L Storm Pants that makes them more breathable and waterproof?',\n",
              " 'answer': 'The EcoFlex 3L Storm Pants feature TEK O2 technology, which offers the most breathability ever tested and ensures waterproof protection.'}"
            ]
          },
          "metadata": {},
          "execution_count": 309
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# to see the details and debug\n",
        "langchain.debug = True\n",
        "\n",
        "qa.run(qa_examples[0][\"query\"])\n",
        "\n",
        "langchain.debug = False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xfdD84B_NGYq",
        "outputId": "7ac789db-c63d-4c7c-8c3c-08aee2c517f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RetrievalQA] Entering Chain run with input:\n",
            "\u001b[0m{\n",
            "  \"query\": \"Do the Cozy Comfort Pullover Set have side pockets?\"\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RetrievalQA > 3:chain:StuffDocumentsChain] Entering Chain run with input:\n",
            "\u001b[0m[inputs]\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RetrievalQA > 3:chain:StuffDocumentsChain > 4:chain:LLMChain] Entering Chain run with input:\n",
            "\u001b[0m{\n",
            "  \"question\": \"Do the Cozy Comfort Pullover Set have side pockets?\",\n",
            "  \"context\": \": 10\\nUnnamed: 0: 10\\nname: Cozy Comfort Pullover Set, Stripe\\ndescription: Perfect for lounging, this striped knit set lives up to its name. We used ultrasoft fabric and an easy design that's as comfortable at bedtime as it is when we have to make a quick run out.\\n\\nSize & Fit\\n- Pants are Favorite Fit: Sits lower on the waist.\\n- Relaxed Fit: Our most generous fit sits farthest from the body.\\n\\nFabric & Care\\n- In the softest blend of 63% polyester, 35% rayon and 2% spandex.\\n\\nAdditional Features\\n- Relaxed fit top with raglan sleeves and rounded hem.\\n- Pull-on pants have a wide elastic waistband and drawstring, side pockets and a modern slim leg.\\n\\nImported.<<<<>>>>>: 73\\nUnnamed: 0: 73\\nname: Cozy Cuddles Knit Pullover Set\\ndescription: Perfect for lounging, this knit set lives up to its name. We used ultrasoft fabric and an easy design that's as comfortable at bedtime as it is when we have to make a quick run out. \\n\\nSize & Fit \\nPants are Favorite Fit: Sits lower on the waist. \\nRelaxed Fit: Our most generous fit sits farthest from the body. \\n\\nFabric & Care \\nIn the softest blend of 63% polyester, 35% rayon and 2% spandex.\\n\\nAdditional Features \\nRelaxed fit top with raglan sleeves and rounded hem. \\nPull-on pants have a wide elastic waistband and drawstring, side pockets and a modern slim leg. \\nImported.<<<<>>>>>: 265\\nUnnamed: 0: 265\\nname: Cozy Workout Vest\\ndescription: For serious warmth that won't weigh you down, reach for this fleece-lined vest, which provides you with layering options whether you're inside or outdoors.\\nSize & Fit\\nRelaxed Fit. Falls at hip.\\nFabric & Care\\nSoft, textured fleece lining. Nylon shell. Machine wash and dry. \\nAdditional Features \\nTwo handwarmer pockets. Knit side panels stretch for a more flattering fit. Shell fabric is treated to resist water and stains. Imported.<<<<>>>>>Additional Features\\n- Comfortable elastic waistband with drawcord.\\n- Back zip pocket for storing keys or cards.\\n- Two hand pockets with mesh pocket bags for easy drainage.\\n- Packs down small and stows away into its own pocket.\\n- Imported.\"\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:RetrievalQA > 3:chain:StuffDocumentsChain > 4:chain:LLMChain > 5:llm:ChatOpenAI] Entering LLM run with input:\n",
            "\u001b[0m{\n",
            "  \"prompts\": [\n",
            "    \"System: Use the following pieces of context to answer the user's question. \\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\\n----------------\\n: 10\\nUnnamed: 0: 10\\nname: Cozy Comfort Pullover Set, Stripe\\ndescription: Perfect for lounging, this striped knit set lives up to its name. We used ultrasoft fabric and an easy design that's as comfortable at bedtime as it is when we have to make a quick run out.\\n\\nSize & Fit\\n- Pants are Favorite Fit: Sits lower on the waist.\\n- Relaxed Fit: Our most generous fit sits farthest from the body.\\n\\nFabric & Care\\n- In the softest blend of 63% polyester, 35% rayon and 2% spandex.\\n\\nAdditional Features\\n- Relaxed fit top with raglan sleeves and rounded hem.\\n- Pull-on pants have a wide elastic waistband and drawstring, side pockets and a modern slim leg.\\n\\nImported.<<<<>>>>>: 73\\nUnnamed: 0: 73\\nname: Cozy Cuddles Knit Pullover Set\\ndescription: Perfect for lounging, this knit set lives up to its name. We used ultrasoft fabric and an easy design that's as comfortable at bedtime as it is when we have to make a quick run out. \\n\\nSize & Fit \\nPants are Favorite Fit: Sits lower on the waist. \\nRelaxed Fit: Our most generous fit sits farthest from the body. \\n\\nFabric & Care \\nIn the softest blend of 63% polyester, 35% rayon and 2% spandex.\\n\\nAdditional Features \\nRelaxed fit top with raglan sleeves and rounded hem. \\nPull-on pants have a wide elastic waistband and drawstring, side pockets and a modern slim leg. \\nImported.<<<<>>>>>: 265\\nUnnamed: 0: 265\\nname: Cozy Workout Vest\\ndescription: For serious warmth that won't weigh you down, reach for this fleece-lined vest, which provides you with layering options whether you're inside or outdoors.\\nSize & Fit\\nRelaxed Fit. Falls at hip.\\nFabric & Care\\nSoft, textured fleece lining. Nylon shell. Machine wash and dry. \\nAdditional Features \\nTwo handwarmer pockets. Knit side panels stretch for a more flattering fit. Shell fabric is treated to resist water and stains. Imported.<<<<>>>>>Additional Features\\n- Comfortable elastic waistband with drawcord.\\n- Back zip pocket for storing keys or cards.\\n- Two hand pockets with mesh pocket bags for easy drainage.\\n- Packs down small and stows away into its own pocket.\\n- Imported.\\nHuman: Do the Cozy Comfort Pullover Set have side pockets?\"\n",
            "  ]\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:RetrievalQA > 3:chain:StuffDocumentsChain > 4:chain:LLMChain > 5:llm:ChatOpenAI] [539ms] Exiting LLM run with output:\n",
            "\u001b[0m{\n",
            "  \"generations\": [\n",
            "    [\n",
            "      {\n",
            "        \"text\": \"Yes, the Cozy Comfort Pullover Set does have side pockets.\",\n",
            "        \"generation_info\": {\n",
            "          \"finish_reason\": \"stop\",\n",
            "          \"logprobs\": null\n",
            "        },\n",
            "        \"type\": \"ChatGeneration\",\n",
            "        \"message\": {\n",
            "          \"lc\": 1,\n",
            "          \"type\": \"constructor\",\n",
            "          \"id\": [\n",
            "            \"langchain\",\n",
            "            \"schema\",\n",
            "            \"messages\",\n",
            "            \"AIMessage\"\n",
            "          ],\n",
            "          \"kwargs\": {\n",
            "            \"content\": \"Yes, the Cozy Comfort Pullover Set does have side pockets.\",\n",
            "            \"additional_kwargs\": {}\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "    ]\n",
            "  ],\n",
            "  \"llm_output\": {\n",
            "    \"token_usage\": {\n",
            "      \"completion_tokens\": 14,\n",
            "      \"prompt_tokens\": 581,\n",
            "      \"total_tokens\": 595\n",
            "    },\n",
            "    \"model_name\": \"gpt-3.5-turbo\",\n",
            "    \"system_fingerprint\": \"fp_2b778c6b35\"\n",
            "  },\n",
            "  \"run\": null\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RetrievalQA > 3:chain:StuffDocumentsChain > 4:chain:LLMChain] [544ms] Exiting Chain run with output:\n",
            "\u001b[0m{\n",
            "  \"text\": \"Yes, the Cozy Comfort Pullover Set does have side pockets.\"\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RetrievalQA > 3:chain:StuffDocumentsChain] [551ms] Exiting Chain run with output:\n",
            "\u001b[0m{\n",
            "  \"output_text\": \"Yes, the Cozy Comfort Pullover Set does have side pockets.\"\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RetrievalQA] [689ms] Exiting Chain run with output:\n",
            "\u001b[0m{\n",
            "  \"result\": \"Yes, the Cozy Comfort Pullover Set does have side pockets.\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# generate predictions for each question\n",
        "predictions = qa.apply(qa_examples) # takes 'query' from the dictionaries automatically"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LoLotN5zO8a4",
        "outputId": "3bc14933-6d39-47cf-df6e-1ff090af520c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate whether the predicted answers are true or false according to the real answers\n",
        "# use another LLM to compare the semantic alignment between two strings\n",
        "eval_chain = QAEvalChain.from_llm(ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo\"))\n",
        "\n",
        "graded_outputs = eval_chain.evaluate(qa_examples, predictions)"
      ],
      "metadata": {
        "id": "x3inaFFxP7Md"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, (prediction, graded_output) in enumerate(zip(predictions, graded_outputs)):\n",
        "    print(f\"Example {i}:\")\n",
        "    print(\"Question: \" + prediction['query'])\n",
        "    print(\"Real Answer: \" + prediction['answer'])\n",
        "    print(\"Predicted Answer: \" + prediction['result'])\n",
        "    print(\"Predicted Grade: \" + graded_output['results'])\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P8ym2_4kRbTK",
        "outputId": "a74559e2-16e7-494d-c5db-f33e86d2c308"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example 0:\n",
            "Question: Do the Cozy Comfort Pullover Set have side pockets?\n",
            "Real Answer: Yes\n",
            "Predicted Answer: Yes, the Cozy Comfort Pullover Set does have side pockets.\n",
            "Predicted Grade: CORRECT\n",
            "\n",
            "Example 1:\n",
            "Question: What collection is the Ultra-Lofty 850 Stretch Down Hooded Jacket from?\n",
            "Real Answer: The DownTek collection\n",
            "Predicted Answer: The Ultra-Lofty 850 Stretch Down Hooded Jacket is from the DownTek collection.\n",
            "Predicted Grade: CORRECT\n",
            "\n",
            "Example 2:\n",
            "Question: What are the key features of the Women's Campside Oxfords as described in the document?\n",
            "Real Answer: The key features of the Women's Campside Oxfords include a super-soft canvas material for a broken-in feel, thick cushioning for comfort, quality construction, comfortable EVA innersole with Cleansport NXT® antimicrobial odor control, vintage hunt, fish and camping motif on the innersole, moderate arch contour, EVA foam midsole for cushioning and support, and a chain-tread-inspired molded rubber outsole with a modified chain-tread pattern.\n",
            "Predicted Answer: The key features of the Women's Campside Oxfords as described in the document are:\n",
            "1. Soft canvas material for a broken-in feel and look.\n",
            "2. Comfortable EVA innersole with Cleansport NXT® antimicrobial odor control.\n",
            "3. Vintage hunt, fish, and camping motif on innersole.\n",
            "4. Moderate arch contour of innersole.\n",
            "5. EVA foam midsole for cushioning and support.\n",
            "6. Chain-tread-inspired molded rubber outsole with modified chain-tread pattern.\n",
            "7. Approx. weight: 1 lb. 1 oz. per pair.\n",
            "Predicted Grade: CORRECT\n",
            "\n",
            "Example 3:\n",
            "Question: What are the dimensions of the small and medium sizes for the Recycled Waterhog Dog Mat, Chevron Weave?\n",
            "Real Answer: The small size has dimensions of 18\" x 28\" and the medium size has dimensions of 22.5\" x 34.5\".\n",
            "Predicted Answer: The dimensions for the small size of the Recycled Waterhog Dog Mat, Chevron Weave are 18\" x 28\", and for the medium size, they are 22.5\" x 34.5\".\n",
            "Predicted Grade: CORRECT\n",
            "\n",
            "Example 4:\n",
            "Question: What are some key features of the Infant and Toddler Girls' Coastal Chill Swimsuit, Two-Piece as described in the document?\n",
            "Real Answer: The key features of the Infant and Toddler Girls' Coastal Chill Swimsuit, Two-Piece include bright colors, ruffles, exclusive whimsical prints, four-way-stretch and chlorine-resistant fabric, UPF 50+ rated fabric for sun protection, crossover no-slip straps, fully lined bottom for secure fit and maximum coverage, and the recommendation to machine wash and line dry for best results.\n",
            "Predicted Answer: Some key features of the Infant and Toddler Girls' Coastal Chill Swimsuit, Two-Piece are:\n",
            "\n",
            "1. Bright colors, ruffles, and exclusive whimsical prints.\n",
            "2. Four-way-stretch and chlorine-resistant fabric.\n",
            "3. UPF 50+ rated fabric for high sun protection.\n",
            "4. Crossover no-slip straps for a secure fit.\n",
            "5. Fully lined bottom for maximum coverage.\n",
            "6. Machine washable and line dry for best results.\n",
            "Predicted Grade: CORRECT\n",
            "\n",
            "Example 5:\n",
            "Question: What is the composition of the fabric used in the Refresh Swimwear, V-Neck Tankini Contrasts?\n",
            "Real Answer: The fabric of the Refresh Swimwear, V-Neck Tankini Contrasts is made of 82% recycled nylon and 18% Lycra® spandex for the body, and 90% recycled nylon and 10% Lycra® spandex for the lining.\n",
            "Predicted Answer: The fabric used in the Refresh Swimwear, V-Neck Tankini Contrasts is composed of 82% recycled nylon with 18% Lycra® spandex for the body, and 90% recycled nylon with 10% Lycra® spandex for the lining.\n",
            "Predicted Grade: CORRECT\n",
            "\n",
            "Example 6:\n",
            "Question: What technology is featured in the EcoFlex 3L Storm Pants that makes them more breathable and waterproof?\n",
            "Real Answer: The EcoFlex 3L Storm Pants feature TEK O2 technology, which offers the most breathability ever tested and ensures waterproof protection.\n",
            "Predicted Answer: The EcoFlex 3L Storm Pants feature TEK O2 technology, which makes them more breathable and waterproof.\n",
            "Predicted Grade: CORRECT\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Agents"
      ],
      "metadata": {
        "id": "OI2ptmkKorF3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@tool # custom tool (docstring helps LLM to determine when to use this tool)\n",
        "def get_time(text: str) -> str:\n",
        "  \"\"\"Returns todays date, use this for any questions related to knowing todays date. \\\n",
        "  The input should always be an empty string, and this function will always return todays \\\n",
        "  date - any date mathmatics should occur outside this function.\n",
        "  \"\"\"\n",
        "  return str(date.today())\n",
        "\n",
        "llm = ChatOpenAI(temperature=0) # LLMs as precise reasoning engines\n",
        "tools = load_tools([\"llm-math\", \"wikipedia\"], llm=llm) + [PythonREPLTool(), get_time]\n",
        "# llm-math tool: a chain (LLM + calculator) solves math problems\n",
        "# wikipedia tool: API that allows to run search queries against Wikipedia and get back results\n",
        "# Python REPL tool: a way to interact with code and run it, the interface will only return things that are printed - therefore, it is important to make sure have it print out the answer\n",
        "prompt = hub.pull(\"hwchase17/openai-functions-agent\")\n",
        "\n",
        "agent = create_openai_functions_agent(llm, tools, prompt) # chain of thought reasoning + action planning = ReAct (generate reasoning traces and task-specific actions, leveraging the synergy)\n",
        "agent_executor = AgentExecutor.from_agent_and_tools(\n",
        "  agent=agent,\n",
        "  tools=tools,\n",
        "  handle_parsing_errors=True, # when the output can't be parsed as desired, it is passed back to LLM to correct itself\n",
        "  verbose=True\n",
        ")"
      ],
      "metadata": {
        "id": "I5rDXKVjsbUq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent_executor.invoke({\"input\": \"What is the answer to the math problem: %25 of 300?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wcqF8e6Cw2bu",
        "outputId": "88476895-4f49-4978-e9d3-c28ff2e3edae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `Calculator` with `25% of 300`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[36;1m\u001b[1;3mAnswer: 75.0\u001b[0m\u001b[32;1m\u001b[1;3mThe answer to the math problem is 75.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'What is the answer to the math problem: %25 of 300?',\n",
              " 'output': 'The answer to the math problem is 75.'}"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent_executor.invoke({\"input\": \"Who is the founder of the Turkish Republic?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tUePxNJRlCjp",
        "outputId": "df481302-32a5-4f61-d093-11adaad58920"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `wikipedia` with `Founder of the Turkish Republic`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[33;1m\u001b[1;3mPage: Kamal (name)\n",
            "Summary: Kamal is a male given name used in several languages. \n",
            "\n",
            "In Sanskrit, it is usually spelled Kamal for males and Kamala for females, meaning \"lotus\" or \"pale red\".\n",
            "Kamal or Kamaal (Arabic: كمال‌ kamāl) or Turkish Kemal. The Arabic name which is also a noun means \"perfection, superiority, distinction\" and \"completion, conclusion, accomplishment\". The name bears the notion of \"completeness of a thing without any deficiency\" and \"perfection of morals and ethics (adjective: اِكْتِمال iktimāl)\". Also the name may be used as an abbreviation of Kamal ad-Din.\n",
            "In Persian, it means \"beauty, perfection, excellence, completion, utmost level\".\n",
            "Azerbaijanis use it as a male name in the meaning of \"competent, mature\".\n",
            "In Turkish, it is the misspelling of Kamâl which means \"siege, blockade, encirclement\" (from the Uzbek qamal) and \"castle, rampart\" (from the Kazakh qamal).\n",
            "In Turkish transliteration of Arabic and Persian name كمال, it is sometimes used instead of Kemal. According to Nişanyan Dictionary, most of the parents who named their children Kamal in Hatay Province of Turkey adopted the Arabic spelling of Kemal.\n",
            "\n",
            "Page: Turkey\n",
            "Summary: Turkey, officially the Republic of Türkiye (Turkish: Türkiye Cumhuriyeti [ˈtyɾcije dʒumˈhuːɾijeti] ), is a country mainly in Anatolia in West Asia, with a smaller part called East Thrace in Southeast Europe. It borders the Black Sea to the north; Georgia, Armenia, Azerbaijan, and Iran to the east; Iraq, Syria, and the Mediterranean Sea (and Cyprus) to the south; and the Aegean Sea, Greece, and Bulgaria to the west. Turkey is home to over 85 million people; most are ethnic Turks, while Kurds are the largest ethnic minority. Officially a secular state, Turkey has a Muslim-majority population. Ankara is Turkey's capital and second-largest city; Istanbul is its largest city, and economic and financial center. Other major cities include İzmir, Bursa, Antalya, and Adana.\n",
            "Human habitation began in Late Paleolithic. Home to important Neolithic sites like Göbekli Tepe and some of the earliest farming areas, present-day Turkey was inhabited by various ancient peoples. Hattians were assimilated by the incoming Anatolian peoples. Increasing diversity during Classical Anatolia transitioned into cultural Hellenization following the conquests of Alexander the Great; Hellenization continued during the Roman and Byzantine eras. The Seljuk Turks began migrating into Anatolia in the 11th century, starting the Turkification process. The Seljuk Sultanate of Rum ruled Anatolia until the Mongol invasion in 1243, when it disintegrated into Turkish principalities. Beginning in 1299, the Ottomans united the principalities and expanded; Mehmed II conquered Istanbul in 1453. During the reigns of Selim I and Suleiman the Magnificent, the Ottoman Empire became a global power.From the late 18th century onwards, the empire's power and territory declined; reforms were also made. In the 19th and early 20th centuries, persecution of Muslims during the Ottoman contraction and in the Russian Empire resulted in large-scale loss of life and mass migration into modern-day Turkey from the Balkans, Caucasus, and Crimea. Second Constitutional Era ended with the 1913 coup d'état. Under the control of Three Pashas, the Ottoman Empire entered World War I in 1914. During the war, the Ottoman government committed genocides against its Armenian, Greek and Assyrian subjects. After its defeat, the Ottoman Empire was partitioned. The Turkish War of Independence resulted in the abolition of the sultanate in 1922 and the signing of the Treaty of Lausanne in 1923. The Republic was proclaimed on 29 October 1923, modelled on the reforms initiated by the country's first president, Mustafa Kemal Atatürk.\n",
            "Turkey is an upper-middle-income and emerging country; its economy is 17th or 11th-largest in the world. It is a unitary presidential republic with a multi-party system. Turkey is a founding member of the OECD, G20, and Organization of Tu\u001b[0m\u001b[32;1m\u001b[1;3mThe founder of the Turkish Republic is Mustafa Kemal Atatürk. He initiated the reforms that led to the establishment of the Republic of Turkey on October 29, 1923.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'Who is the founder of the Turkish Republic?',\n",
              " 'output': 'The founder of the Turkish Republic is Mustafa Kemal Atatürk. He initiated the reforms that led to the establishment of the Republic of Turkey on October 29, 1923.'}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "customer_list = [[\"Harrison\", \"Chase\"], [\"Lang\", \"Chain\"], [\"Dolly\", \"Too\"], [\"Elle\", \"Elem\"], [\"Geoff\",\"Fusion\"], [\"Trance\",\"Former\"], [\"Jen\",\"Ayai\"]]\n",
        "agent_executor.invoke({\"input\": f\"\"\"Sort these customers by last name and then first name and then use the print function to list the sorted as output: {customer_list}\"\"\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EKOU5OswmXEZ",
        "outputId": "4e8b5400-4405-4c28-f385-a2594ba12edd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `Python_REPL` with `customers = [['Harrison', 'Chase'], ['Lang', 'Chain'], ['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Jen', 'Ayai']]\n",
            "sorted_customers = sorted(customers, key=lambda x: (x[1], x[0]))\n",
            "print(sorted_customers)`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[38;5;200m\u001b[1;3m[['Jen', 'Ayai'], ['Lang', 'Chain'], ['Harrison', 'Chase'], ['Elle', 'Elem'], ['Trance', 'Former'], ['Geoff', 'Fusion'], ['Dolly', 'Too']]\n",
            "\u001b[0m\u001b[32;1m\u001b[1;3mThe customers sorted by last name and then first name are:\n",
            "1. ['Jen', 'Ayai']\n",
            "2. ['Lang', 'Chain']\n",
            "3. ['Harrison', 'Chase']\n",
            "4. ['Elle', 'Elem']\n",
            "5. ['Trance', 'Former']\n",
            "6. ['Geoff', 'Fusion']\n",
            "7. ['Dolly', 'Too']\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': \"Sort these customers by last name and then first name and then use the print function to list the sorted as output: [['Harrison', 'Chase'], ['Lang', 'Chain'], ['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Jen', 'Ayai']]\",\n",
              " 'output': \"The customers sorted by last name and then first name are:\\n1. ['Jen', 'Ayai']\\n2. ['Lang', 'Chain']\\n3. ['Harrison', 'Chase']\\n4. ['Elle', 'Elem']\\n5. ['Trance', 'Former']\\n6. ['Geoff', 'Fusion']\\n7. ['Dolly', 'Too']\"}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent_executor.invoke({\"input\": \"whats the date today?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fH4N54crzmJP",
        "outputId": "a649e668-8d52-45fa-d288-08d230e9bb92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `get_time` with `{'text': ''}`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[36;1m\u001b[1;3m2024-03-04\u001b[0m\u001b[32;1m\u001b[1;3mToday's date is March 4, 2024.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'whats the date today?', 'output': \"Today's date is March 4, 2024.\"}"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RAG"
      ],
      "metadata": {
        "id": "Had18g6IP_7y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Document Loading"
      ],
      "metadata": {
        "id": "Hb2fvIvhQFbG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# accessing and reading different types of data as standard document format\n",
        "loader = PyPDFLoader(\"./drive/MyDrive/dataset/docs/pdf/A Fast, Minimal Memory, Consistent Hash Algorithm (1406.2294).pdf\")\n",
        "pages = loader.load() # a list of documents (each page is a unique document)\n",
        "page = pages[0] # document\n",
        "\n",
        "print(page.page_content[:100]) # content of the page\n",
        "print(page.metadata) # metadata associated with each document"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6GVciqSEQDID",
        "outputId": "39f10d91-fe3d-4cd0-dc1e-1854cb01fa0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A Fast, Minimal Memory, Consistent Hash Algorithm \n",
            " \n",
            "John Lamping, Eric Veach \n",
            "Google \n",
            " \n",
            "Abstract \n",
            " \n",
            "{'source': './drive/MyDrive/dataset/docs/pdf/A Fast, Minimal Memory, Consistent Hash Algorithm (1406.2294).pdf', 'page': 0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loader = WebBaseLoader(\"https://github.com/basecamp/handbook/blob/master/37signals-is-you.md\")\n",
        "docs = loader.load()"
      ],
      "metadata": {
        "id": "ZII2mrfbix61"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# YouTube transcripts\n",
        "loader = YoutubeLoader.from_youtube_url(\n",
        "  \"https://www.youtube.com/watch?v=QsYGlZkevEg\",\n",
        "  add_video_info=True,\n",
        "  language=[\"en\", \"id\"],\n",
        "  translation=\"en\"\n",
        ")\n",
        "docs = loader.load()"
      ],
      "metadata": {
        "id": "P95HyBOvcize"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Document Splitting"
      ],
      "metadata": {
        "id": "KKTMrVQHkFbX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# splitting documents into smaller chunks before storing them in vector stores\n",
        "# chunks should include semanticly relevant and complete sentences!\n",
        "\n",
        "text = \"\"\"When writing documents, writers will use document structure to group content. \\\n",
        "This can convey to the reader, which idea's are related. For example, closely related ideas \\\n",
        "are in sentances. Similar ideas are in paragraphs. Paragraphs form a document.\\n\\n\\\n",
        "Paragraphs are often delimited with a carriage return or two carriage returns. \\\n",
        "Carriage returns are the \"backslash n\" you see embedded in this string. \\\n",
        "Sentences have a period at the end, but also, have a space \\\n",
        "and words are separated by space.\"\"\"\n",
        "\n",
        "loader = TextLoader(\"drive/MyDrive/dataset/docs/md/our-rituals.md\")\n",
        "docs = loader.load()"
      ],
      "metadata": {
        "id": "ghQ_NcOgv7t0"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# splitters try to limit the size of the chunks as close as to the maximum size\n",
        "# by combining splits according to the separators\n",
        "# while keeping some overlap content from the previous chunk with respect to the maximum overlap size\n",
        "\n",
        "splitter = CharacterTextSplitter(\n",
        "  chunk_size=26,\n",
        "  chunk_overlap=4,\n",
        "  separator=\"\",\n",
        "  length_function=len\n",
        ")\n",
        "\n",
        "splits = splitter.split_text(text) # splits text into list of text chunks\n",
        "documents = splitter.create_documents([text]) # splits list of texts and those splits into documents\n",
        "chunks = splitter.split_documents(docs) # splits list of documents into chunks"
      ],
      "metadata": {
        "id": "eitZlPbDRI-K"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# in case of a exceeding chunk size, next separator is taken into consideration\n",
        "\n",
        "splitter = RecursiveCharacterTextSplitter(\n",
        "  chunk_size=26,\n",
        "  chunk_overlap=4,\n",
        "  separators=[\"\\n\\n\", \"\\n\", r\"(?<=\\. )\", \"!\", \"?\", \",\", \" \", \"\"], # use look-behind regex to fix separator at the end of the split\n",
        "  keep_separator=True\n",
        ")\n",
        "\n",
        "splits = splitter.split_text(text)\n",
        "documents = splitter.create_documents([text])\n",
        "chunks = splitter.split_documents(docs)"
      ],
      "metadata": {
        "id": "rF5VFaR-RUm0"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# useful when considering the context window size of LLMs\n",
        "# tokens are usually ~4 characters\n",
        "\n",
        "splitter = TokenTextSplitter(\n",
        "  chunk_size=26,\n",
        "  chunk_overlap=4,\n",
        "  # encoding name or model name should be specified\n",
        "  encoding_name=\"gpt2\",\n",
        "  # model_name=\"gpt2\",\n",
        ")\n",
        "# CharacterTextSplitter.from_tiktoken_encoder(encoding_name=\"gpt2\",)\n",
        "\n",
        "splits = splitter.split_text(text)\n",
        "documents = splitter.create_documents([text])\n",
        "chunks = splitter.split_documents(docs)"
      ],
      "metadata": {
        "id": "36sUkGtxRXhy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# splitting based on specific headers to keep common context together\n",
        "# also adds information to metadata for each chunk\n",
        "\n",
        "splitter = MarkdownHeaderTextSplitter(\n",
        "  headers_to_split_on=[\n",
        "    (\"#\", \"Header 1\"),\n",
        "    (\"##\", \"Header 2\"),\n",
        "    (\"###\", \"Header 3\")\n",
        "  ]\n",
        ")\n",
        "\n",
        "splits = splitter.split_text(' '.join([d.page_content for d in docs]))"
      ],
      "metadata": {
        "id": "mJHLYP_7RcBF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# allows splitting code for supported programming languages\n",
        "\n",
        "# RecursiveCharacterTextSplitter.from_language(language=Language.CSHARP,)"
      ],
      "metadata": {
        "id": "WrF1AiEKReqM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# splitting text by looking at sentences according to the rules of the language\n",
        "\n",
        "# NLTKTextSplitter(language=\"english\",)\n",
        "# SpacyTextSplitter(pipeline=\"en_core_web_sm\",)"
      ],
      "metadata": {
        "id": "p0fzTWnJRhHt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vectorstores and Embedding"
      ],
      "metadata": {
        "id": "9-H3SawHJbxP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loaders = [\n",
        "  # duplicate documents on purpose - messy data\n",
        "  TextLoader(\"drive/MyDrive/dataset/docs/md/our-rituals.md\"),\n",
        "  TextLoader(\"drive/MyDrive/dataset/docs/md/our-rituals.md\"),\n",
        "  TextLoader(\"drive/MyDrive/dataset/docs/md/benefits-and-perks.md\")\n",
        "]\n",
        "docs = []\n",
        "for loader in loaders:\n",
        "  docs.extend(loader.load())\n",
        "\n",
        "splitter = RecursiveCharacterTextSplitter(\n",
        "  chunk_size = 200,\n",
        "  chunk_overlap = 20\n",
        ")\n",
        "splits = splitter.split_documents(docs)"
      ],
      "metadata": {
        "id": "iLST8p5uJeRp"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# numerical representations of text to find text with similar context or semantic meaning\n",
        "\n",
        "sentence1 = \"i like dogs\"\n",
        "sentence2 = \"i like canines\"\n",
        "sentence3 = \"the weather is ugly outside\"\n",
        "\n",
        "embed = OpenAIEmbeddings()\n",
        "\n",
        "embedding1 = embed.embed_query(sentence1)\n",
        "embedding2 = embed.embed_query(sentence2)\n",
        "embedding3 = embed.embed_query(sentence3)\n",
        "\n",
        "print(np.dot(embedding1, embedding2))\n",
        "print(np.dot(embedding1, embedding3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0-5_ffT4NA76",
        "outputId": "15e17968-96cf-4629-c333-54277a1fc0d5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9631227500523626\n",
            "0.7703257495981698\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "persist_directory = 'chroma/'\n",
        "!rm -rf ./chroma  # remove old database files if any\n",
        "\n",
        "# a database which stores embeddings and allows looking up for similar vectors\n",
        "vectordb = Chroma.from_documents(\n",
        "    documents=splits,\n",
        "    embedding=embed,\n",
        "    persist_directory=persist_directory\n",
        ")\n",
        "\n",
        "print(vectordb._collection.count()) # number of individual splits = len(splits)\n",
        "\n",
        "vectordb.persist() # persist the vector database for future use"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9KvcgJx2N4tm",
        "outputId": "376702c2-515e-4045-ccf1-2e27b7864698"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "133\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# basic semantic search\n",
        "\n",
        "question = \"When does an All Hands meeting occur?\"\n",
        "retrieved_docs = vectordb.similarity_search(question, k=3) # k = len(retrieved_docs)\n",
        "retrieved_docs\n",
        "# retrieved_docs[0].page_content\n",
        "# retrieved_docs[0].metadata"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iY6ad6irPV1t",
        "outputId": "0fae15cc-5b9d-4164-b278-f4f1a39de460"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='At the end of every cycle, we hold an All Hands meeting. Everyone at the company gathers on a Zoom call (or in person, at meetups) to hear about product development, business operations, new hires,', metadata={'source': 'drive/MyDrive/dataset/docs/md/our-rituals.md'}),\n",
              " Document(page_content='At the end of every cycle, we hold an All Hands meeting. Everyone at the company gathers on a Zoom call (or in person, at meetups) to hear about product development, business operations, new hires,', metadata={'source': 'drive/MyDrive/dataset/docs/md/our-rituals.md'}),\n",
              " Document(page_content='## All Hands', metadata={'source': 'drive/MyDrive/dataset/docs/md/our-rituals.md'})]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retrieval"
      ],
      "metadata": {
        "id": "apJ2t5T9XqSx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texts = [\n",
        "  \"\"\"The Amanita phalloides has a large and imposing epigeous (aboveground) fruiting body (basidiocarp).\"\"\",\n",
        "  \"\"\"A mushroom with a large fruiting body is the Amanita phalloides. Some varieties are all-white.\"\"\",\n",
        "  \"\"\"A. phalloides, a.k.a Death Cap, is one of the most poisonous of all known mushrooms.\"\"\",\n",
        "]\n",
        "vectordb_small = Chroma.from_texts(texts, embedding=OpenAIEmbeddings())\n",
        "question = \"Tell me about all-white mushrooms with large fruiting bodies\""
      ],
      "metadata": {
        "id": "8GbWAEdpXurh"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectordb_small.similarity_search(question, k=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dc3ewJMOZffp",
        "outputId": "8b00730f-6be2-4469-c453-36a9a27261d5"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='A mushroom with a large fruiting body is the Amanita phalloides. Some varieties are all-white.'),\n",
              " Document(page_content='The Amanita phalloides has a large and imposing epigeous (aboveground) fruiting body (basidiocarp).')]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# maximum marginal relevance strives to achieve\n",
        "# both semantically relevant (relevance to the query)\n",
        "# and distinct (diverse among the results) chunks\n",
        "\n",
        "vectordb_small.max_marginal_relevance_search(question, k=2, fetch_k=3) # from top fetch_k most relevant documents, return top k most diverse documents"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AsCnyIa7Zk3K",
        "outputId": "00283d63-ad33-49de-a0ab-e0c216a1743d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='A mushroom with a large fruiting body is the Amanita phalloides. Some varieties are all-white.'),\n",
              " Document(page_content='A. phalloides, a.k.a Death Cap, is one of the most poisonous of all known mushrooms.')]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectordb = Chroma(\n",
        "  persist_directory=\"chroma/\",\n",
        "  embedding_function=OpenAIEmbeddings()\n",
        ")\n",
        "question = \"when does the all hands meeting, one of the company's rituals, take place?\"\n",
        "vectordb.similarity_search(\n",
        "  question,\n",
        "  k=3,\n",
        "  # metadata filtering based on structured information that is hard to capture semantically\n",
        "  filter={'source': 'drive/MyDrive/dataset/docs/md/our-rituals.md'}\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g3IV0E_LayfU",
        "outputId": "12dda908-7965-4e7b-8c21-10a529f61b23"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='At the end of every cycle, we hold an All Hands meeting. Everyone at the company gathers on a Zoom call (or in person, at meetups) to hear about product development, business operations, new hires,', metadata={'source': 'drive/MyDrive/dataset/docs/md/our-rituals.md'}),\n",
              " Document(page_content='At the end of every cycle, we hold an All Hands meeting. Everyone at the company gathers on a Zoom call (or in person, at meetups) to hear about product development, business operations, new hires,', metadata={'source': 'drive/MyDrive/dataset/docs/md/our-rituals.md'}),\n",
              " Document(page_content='## All Hands', metadata={'source': 'drive/MyDrive/dataset/docs/md/our-rituals.md'})]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "metadata_field_info = [\n",
        "  AttributeInfo(\n",
        "    name=\"source\",\n",
        "    description=\"The document name where the chunk is from, should be one of `drive/MyDrive/dataset/docs/md/our-rituals.md`, or `drive/MyDrive/dataset/docs/md/benefits-and-perks.md`.\",\n",
        "    type=\"string\"\n",
        "  ),\n",
        "  AttributeInfo(\n",
        "    name=\"page\",\n",
        "    description=\"The page number of the document.\",\n",
        "    type=\"integer\"\n",
        "  )\n",
        "]\n",
        "document_content_description = \"Descriptive documents about company information.\"\n",
        "llm = OpenAI(model='gpt-3.5-turbo-instruct', temperature=0)\n",
        "# inferring the metadata from the query itself\n",
        "retriever = SelfQueryRetriever.from_llm(\n",
        "  llm,\n",
        "  vectordb,\n",
        "  document_content_description,\n",
        "  metadata_field_info,\n",
        "  verbose=True\n",
        ")\n",
        "\n",
        "question = \"when does the all hands meeting, one of the company's rituals, take place?\"\n",
        "retriever.get_relevant_documents(question)\n",
        "# [e async for e in retriever.astream_events(question, version=\"v1\")]"
      ],
      "metadata": {
        "id": "wgqHe6XcW6za"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# contextual compression gets rid of irrelevant text to improve the quality of\n",
        "# retrieved documents by revealing the buried information most relevant to the query\n",
        "# each retrieved document is passed through an LLM and it comes at a cost\n",
        "\n",
        "compression_retriever = ContextualCompressionRetriever(\n",
        "  base_compressor=LLMChainExtractor.from_llm(OpenAI(temperature=0, model=\"gpt-3.5-turbo-instruct\")),\n",
        "  base_retriever=vectordb.as_retriever(search_type=\"mmr\")\n",
        ")\n",
        "\n",
        "question = \"when does the all hands meeting, one of the company's rituals, take place?\"\n",
        "compression_retriever.get_relevant_documents(question)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4R3yopxEjaoG",
        "outputId": "2ee0b3bf-9d06-42db-8bf8-0b5d823ba40a"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='At the end of every cycle, we hold an All Hands meeting.', metadata={'source': 'drive/MyDrive/dataset/docs/md/our-rituals.md'}),\n",
              " Document(page_content='All Hands', metadata={'source': 'drive/MyDrive/dataset/docs/md/our-rituals.md'}),\n",
              " Document(page_content='- Our Rituals\\n- Meet-ups', metadata={'source': 'drive/MyDrive/dataset/docs/md/our-rituals.md'})]"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = SVMRetriever.from_texts(texts, OpenAIEmbeddings())\n",
        "question = \"Tell me about all-white mushrooms with large fruiting bodies\"\n",
        "retriever.get_relevant_documents(question)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fU4vcMgplxS9",
        "outputId": "f5378b93-828d-410e-d796-2abe65a95b5d"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='A mushroom with a large fruiting body is the Amanita phalloides. Some varieties are all-white.'),\n",
              " Document(page_content='The Amanita phalloides has a large and imposing epigeous (aboveground) fruiting body (basidiocarp).'),\n",
              " Document(page_content='A. phalloides, a.k.a Death Cap, is one of the most poisonous of all known mushrooms.')]"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = TFIDFRetriever.from_texts(texts)\n",
        "question = \"Tell me about all-white mushrooms with large fruiting bodies\"\n",
        "retriever.get_relevant_documents(question)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QIKE_eCymXWi",
        "outputId": "e593efc3-2b91-4d48-c590-d0b6bcbac898"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='A mushroom with a large fruiting body is the Amanita phalloides. Some varieties are all-white.'),\n",
              " Document(page_content='A. phalloides, a.k.a Death Cap, is one of the most poisonous of all known mushrooms.'),\n",
              " Document(page_content='The Amanita phalloides has a large and imposing epigeous (aboveground) fruiting body (basidiocarp).')]"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    }
  ]
}
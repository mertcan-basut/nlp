{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "RZtGHoHwraEl",
        "DIGo1r1jsk49",
        "alVvbSct2NvI"
      ],
      "authorship_tag": "ABX9TyM/oboiJnadEk5Kvcu3KxpY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mertcan-basut/nlp/blob/main/langchain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q openai\n",
        "\n",
        "!pip install -q langchain langchain-openai\n",
        "!pip install -q docarray\n",
        "!pip install -q pydantic==1.10.9 # downgrade for pydantic and Langchain compatibility: https://python.langchain.com/docs/guides/pydantic_compatibility\n",
        "\n",
        "!pip install -q python-dotenv"
      ],
      "metadata": {
        "id": "wGmNpMwOKna1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!echo \"OPENAI_API_KEY=editme\" > .env"
      ],
      "metadata": {
        "id": "kZUgLx7JHh6Z"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 248,
      "metadata": {
        "id": "CjpO4-lVFA2z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f15239d-7724-4972-a6a2-3bf6568d714e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# a framework for developing LM powered applications\n",
        "from langchain_openai import ChatOpenAI # models\n",
        "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, SystemMessagePromptTemplate, PromptTemplate # input prompts\n",
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "from langchain.output_parsers import StructuredOutputParser # output parsers\n",
        "from langchain.output_parsers import ResponseSchema\n",
        "from langchain.chains import ConversationChain # memory\n",
        "from langchain.memory import ConversationBufferMemory, ConversationBufferWindowMemory, ConversationTokenBufferMemory, ConversationSummaryBufferMemory\n",
        "from langchain.chains import LLMChain, SimpleSequentialChain, SequentialChain # chains\n",
        "from langchain.chains.router import MultiPromptChain\n",
        "from langchain.chains.router.llm_router import LLMRouterChain, RouterOutputParser\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.document_loaders import CSVLoader\n",
        "from langchain.vectorstores import DocArrayInMemorySearch\n",
        "from langchain.indexes import VectorstoreIndexCreator\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain_community.llms.openai import OpenAI\n",
        "\n",
        "import openai # direct API calls to OpenAI\n",
        "\n",
        "import json\n",
        "import os\n",
        "# https://platform.openai.com/api-keys\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "_ = load_dotenv(find_dotenv()) # read local .env file\n",
        "\n",
        "from IPython.display import display, Markdown\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "# https://s172-31-11-251p14136.lab-aws-production.deeplearning.ai/edit/OutdoorClothingCatalog_1000.csv\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "# /usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Models, Prompts, Parsers"
      ],
      "metadata": {
        "id": "RZtGHoHwraEl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"\\\n",
        "This leaf blower is pretty amazing.  It has four settings: \\\n",
        "candle blower, gentle breeze, windy city, and tornado. \\\n",
        "It arrived in two days, just in time for my wife's \\\n",
        "anniversary present. \\\n",
        "I think my wife liked it so much she was speechless. \\\n",
        "So far I've been the only one using it, and I've been \\\n",
        "using it every other morning to clear the leaves on our lawn. \\\n",
        "It's slightly more expensive than the other leaf blowers \\\n",
        "out there, but I think it's worth it for the extra features.\\\n",
        "\"\"\"\n",
        "\n",
        "system_message_template = \"\"\"\\\n",
        "For the following text, extract the following information:\n",
        "\n",
        "gift: Was the item purchased as a gift for someone else? Answer True if yes, False if not or unknown.\n",
        "\n",
        "delivery_days: How many days did it take for the product to arrive? If this information is not found, output -1.\n",
        "\n",
        "price_value: Extract any sentences about the value or price, and output them as a comma separated Python list.\n",
        "\"\"\"\n",
        "\n",
        "human_message_template = \"\"\"\\\n",
        "text: {text}\n",
        "\"\"\"\n",
        "\n",
        "format_instructions_template = \"\"\"\\\n",
        "Format the output as JSON with the following keys:\n",
        "gift\n",
        "delivery_days\n",
        "price_value\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "CuOOgUWy00AI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client = openai.OpenAI(api_key=os.environ['OPENAI_API_KEY'])\n",
        "\n",
        "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        temperature=0,\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "get_completion(\"Hi!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "ez7AjgGLJ7jq",
        "outputId": "00398d69-ff06-48f4-845e-66a396d9113b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hello! How can I assist you today?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def format_prompt(system_message, human_message, format_instructions):\n",
        "  return f\"\"\"\\\n",
        "{system_message}\n",
        "\n",
        "{human_message}\n",
        "\n",
        "{format_instructions}\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "system_message_prompt = system_message_template.format()\n",
        "human_message_prompt = human_message_template.format(text=text)\n",
        "format_instructions_prompt = format_instructions_template.format()\n",
        "prompt = format_prompt(system_message_prompt, human_message_prompt, format_instructions_prompt)\n",
        "print(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "srwP4Vv99JSI",
        "outputId": "ef3c3065-bde7-464f-9ac3-faa85cefee16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For the following text, extract the following information:\n",
            "\n",
            "gift: Was the item purchased as a gift for someone else? Answer True if yes, False if not or unknown.\n",
            "\n",
            "delivery_days: How many days did it take for the product to arrive? If this information is not found, output -1.\n",
            "\n",
            "price_value: Extract any sentences about the value or price, and output them as a comma separated Python list.\n",
            "\n",
            "\n",
            "text: This leaf blower is pretty amazing.  It has four settings: candle blower, gentle breeze, windy city, and tornado. It arrived in two days, just in time for my wife's anniversary present. I think my wife liked it so much she was speechless. So far I've been the only one using it, and I've been using it every other morning to clear the leaves on our lawn. It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\n",
            "\n",
            "\n",
            "Format the output as JSON with the following keys:\n",
            "gift\n",
            "delivery_days\n",
            "price_value\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = get_completion(prompt)\n",
        "print(response)\n",
        "\n",
        "output_dict = json.loads(response)\n",
        "output_dict.get('delivery_days')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YlXK5Ub0BNOD",
        "outputId": "9e7dc2ef-6603-45a6-fd90-de7de3897ed0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"gift\": true,\n",
            "  \"delivery_days\": 2,\n",
            "  \"price_value\": [\"It's slightly more expensive than the other leaf blowers out there\"]\n",
            "}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 136
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.0)\n",
        "\n",
        "chat([HumanMessage(content=\"Hi!\")]).content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "qlSD8PcbwgSN",
        "outputId": "a73b6afd-0cc9-457e-e3f1-39506162dc02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hello! How can I assist you today?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "system_message_prompt = SystemMessagePromptTemplate.from_template(system_message_template)\n",
        "human_message_prompt = HumanMessagePromptTemplate.from_template(human_message_template)\n",
        "\n",
        "gift_schema = ResponseSchema(name=\"gift\", description=\"Was the item purchased as a gift for someone else? Answer True if yes, False if not or unknown.\")\n",
        "delivery_days_schema = ResponseSchema(name=\"delivery_days\", description=\"How many days did it take for the product to arrive? If this information is not found, output -1.\")\n",
        "price_value_schema = ResponseSchema(name=\"price_value\", description=\"Extract any sentences about the value or price, and output them as a comma separated Python list.\")\n",
        "response_schemas = [gift_schema, delivery_days_schema, price_value_schema]\n",
        "\n",
        "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
        "\n",
        "format_instructions = output_parser.get_format_instructions()\n",
        "format_instructions_prompt = SystemMessage(content=format_instructions)\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt, format_instructions_prompt])\n",
        "prompt = prompt_template.format_messages(text=text)\n",
        "for message in prompt: print(message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-m-vJjDA5ZA",
        "outputId": "aa155e13-0b14-4294-aa19-7c788e054f82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For the following text, extract the following information:\n",
            "\n",
            "gift: Was the item purchased as a gift for someone else? Answer True if yes, False if not or unknown.\n",
            "\n",
            "delivery_days: How many days did it take for the product to arrive? If this information is not found, output -1.\n",
            "\n",
            "price_value: Extract any sentences about the value or price, and output them as a comma separated Python list.\n",
            "\n",
            "text: This leaf blower is pretty amazing.  It has four settings: candle blower, gentle breeze, windy city, and tornado. It arrived in two days, just in time for my wife's anniversary present. I think my wife liked it so much she was speechless. So far I've been the only one using it, and I've been using it every other morning to clear the leaves on our lawn. It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\n",
            "\n",
            "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
            "\n",
            "```json\n",
            "{\n",
            "\t\"gift\": string  // Was the item purchased as a gift for someone else? Answer True if yes, False if not or unknown.\n",
            "\t\"delivery_days\": string  // How many days did it take for the product to arrive? If this information is not found, output -1.\n",
            "\t\"price_value\": string  // Extract any sentences about the value or price, and output them as a comma separated Python list.\n",
            "}\n",
            "```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = chat(prompt).content\n",
        "print(response)\n",
        "\n",
        "output_dict = output_parser.parse(response)\n",
        "output_dict.get('delivery_days')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3gB1eU40xo2",
        "outputId": "6c69f3cd-fd26-4871-9a99-f01b35a2ad7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "```json\n",
            "{\n",
            "\t\"gift\": true,\n",
            "\t\"delivery_days\": 2,\n",
            "\t\"price_value\": \"It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\"\n",
            "}\n",
            "```\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Memory"
      ],
      "metadata": {
        "id": "DIGo1r1jsk49"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(temperature=0.0, model=\"gpt-3.5-turbo\")\n",
        "memory = ConversationBufferMemory()\n",
        "conversation = ConversationChain(llm=llm, memory = memory, verbose=True)"
      ],
      "metadata": {
        "id": "OwVMxqjmsmnm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(conversation.predict(input=\"Hi, my name is Mert.\"))\n",
        "print(conversation.predict(input=\"What is 1+1?\"))\n",
        "print(conversation.predict(input=\"What is my name?\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MWPx4j9UtNcL",
        "outputId": "29383f61-b9bd-4c75-9423-a9ac1363a38c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: Hi, my name is Mert.\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Hello Mert! It's nice to meet you. How can I assist you today?\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Hi, my name is Mert.\n",
            "AI: Hello Mert! It's nice to meet you. How can I assist you today?\n",
            "Human: What is 1+1?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "1+1 equals 2. Is there anything else you would like to know?\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Hi, my name is Mert.\n",
            "AI: Hello Mert! It's nice to meet you. How can I assist you today?\n",
            "Human: What is 1+1?\n",
            "AI: 1+1 equals 2. Is there anything else you would like to know?\n",
            "Human: What is my name?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Your name is Mert. Is there anything else you would like to know or discuss?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(memory.buffer, '\\n')\n",
        "memory.load_memory_variables({})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZybXyJK4t6Yl",
        "outputId": "e9de7485-0da6-4733-b583-2da42c9e7095"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Human: Hi, my name is Mert.\n",
            "AI: Hello Mert! It's nice to meet you. How can I assist you today?\n",
            "Human: What is 1+1?\n",
            "AI: 1 + 1 equals 2. Is there anything else you would like to know?\n",
            "Human: What is my name?\n",
            "AI: Your name is Mert. Is there anything else you would like to know or discuss? \n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': \"Human: Hi, my name is Mert.\\nAI: Hello Mert! It's nice to meet you. How can I assist you today?\\nHuman: What is 1+1?\\nAI: 1 + 1 equals 2. Is there anything else you would like to know?\\nHuman: What is my name?\\nAI: Your name is Mert. Is there anything else you would like to know or discuss?\"}"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# add aditional data to memory\n",
        "memory.save_context({\"input\": \"Hi!\"}, {\"output\": \"What's up?\"})\n",
        "print(memory.buffer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uTgquHyKukqJ",
        "outputId": "522db347-d635-4ee9-d703-e03a3e716e1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Human: Hi, my name is Mert.\n",
            "AI: Hello Mert! It's nice to meet you. How can I assist you today?\n",
            "Human: What is 1+1?\n",
            "AI: 1 + 1 equals 2. Is there anything else you would like to know?\n",
            "Human: What is my name?\n",
            "AI: Your name is Mert. Is there anything else you would like to know or discuss?\n",
            "Human: Hi!\n",
            "AI: What's up?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "memory = ConversationBufferWindowMemory(k=1) # only keeps most recent `k` conversations\n",
        "\n",
        "memory.save_context({\"input\": \"Hi!\"}, {\"output\": \"What's up?\"})\n",
        "memory.save_context({\"input\": \"Not much, just hanging.\"}, {\"output\": \"Cool.\"})\n",
        "\n",
        "print(memory.buffer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gOdw6gLHv8Nf",
        "outputId": "a672f9aa-7de3-45d2-8273-21feeda5f982"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Human: Not much, just hanging.\n",
            "AI: Cool.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=30) # chops off the earlier parts of the conversation to not exceed the token limit dependent on the LLM because usually cost is determined by number of tokens\n",
        "\n",
        "memory.save_context({\"input\": \"AI is what?!\"}, {\"output\": \"Amazing!\"})\n",
        "memory.save_context({\"input\": \"Backpropagation is what?\"}, {\"output\": \"Beautiful!\"})\n",
        "memory.save_context({\"input\": \"Chatbots are what?\"}, {\"output\": \"Charming!\"})\n",
        "\n",
        "print(memory.buffer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OZJ_TWC_w8v3",
        "outputId": "67c8c412-abca-4aa6-9242-36fe2e280fac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AI: Beautiful!\n",
            "Human: Chatbots are what?\n",
            "AI: Charming!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=30) # uses the specified LLM to summarize the entire chat history which exceeds the specified number of tokens\n",
        "\n",
        "memory.save_context({\"input\": \"AI is what?!\"}, {\"output\": \"Amazing!\"})\n",
        "memory.save_context({\"input\": \"Backpropagation is what?\"}, {\"output\": \"Beautiful!\"})\n",
        "memory.save_context({\"input\": \"Chatbots are what?\"}, {\"output\": \"Charming!\"})\n",
        "\n",
        "print(memory.load_memory_variables({})['history'])\n",
        "# `System` is not official OpenAI system message!"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aspHswv2x1IW",
        "outputId": "4a77020d-74bb-44f9-f34d-d971dfa27620"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "System: The human expresses surprise at the AI's positive view of artificial intelligence. The AI responds with \"Amazing!\" and the human asks about backpropagation.\n",
            "AI: Beautiful!\n",
            "Human: Chatbots are what?\n",
            "AI: Charming!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chains"
      ],
      "metadata": {
        "id": "alVvbSct2NvI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(temperature=0.9, model=\"gpt-3.5-turbo\")\n",
        "\n",
        "product = \"Queen Size Sheet Set\"\n",
        "review = \"Je trouve le goût médiocre. La mousse ne tient pas, c'est bizarre. J'achète les mêmes dans le commerce et le goût est bien meilleur...\\nVieux lot ou contrefaçon !?\""
      ],
      "metadata": {
        "id": "8plS0FvW5lnP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = ChatPromptTemplate.from_template(\"What is the best name to describe a company that makes {product}?\")\n",
        "chain = LLMChain(llm=llm, prompt=prompt_template) # LLM + prompt (most basic)\n",
        "\n",
        "chain.run(product)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "QhHkL91v2PCG",
        "outputId": "e1477dda-1c57-4e0d-916e-de3870048e55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Royal Comfort Bedding'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ideal when expecting 1 input & returning 1 output for every chain in sequence (output of the previous chain passed as input into the next chain)\n",
        "prompt_template_1 = ChatPromptTemplate.from_template(\"What is the best name to describe a company that makes {product}?\")\n",
        "chain_1 = LLMChain(llm=llm, prompt=prompt_template_1)\n",
        "\n",
        "prompt_template_2 = ChatPromptTemplate.from_template(\"Write a 20 words description for the following company:{company_name}\")\n",
        "chain_2 = LLMChain(llm=llm, prompt=prompt_template_2)\n",
        "\n",
        "chain = SimpleSequentialChain(chains=[chain_1, chain_2], verbose=True)\n",
        "\n",
        "chain.run(product)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "id": "HIB9ooML5JeZ",
        "outputId": "e44f2ace-3142-4fd7-c32f-08a5cf658fd5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
            "\u001b[36;1m\u001b[1;3mRegal Dreams Co.\u001b[0m\n",
            "\u001b[33;1m\u001b[1;3mRegal Dreams Co. specializes in luxury home decor items, offering elegant and timeless pieces to elevate any living space.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Regal Dreams Co. specializes in luxury home decor items, offering elegant and timeless pieces to elevate any living space.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sequentially executed multiple inputs & multiple outputs (more complicated)\n",
        "prompt_template_1 = ChatPromptTemplate.from_template(\"Translate the following review to english:\\n\\n{Review}\")\n",
        "chain_1 = LLMChain(llm=llm, prompt=prompt_template_1, output_key=\"English_Review\")\n",
        "\n",
        "prompt_template_2 = ChatPromptTemplate.from_template(\"Can you summarize the following review in 1 sentence:\\n\\n{English_Review}\")\n",
        "chain_2 = LLMChain(llm=llm, prompt=prompt_template_2, output_key=\"summary\")\n",
        "\n",
        "prompt_template_3 = ChatPromptTemplate.from_template(\"What language is the following review:\\n\\n{Review}\")\n",
        "chain_3 = LLMChain(llm=llm, prompt=prompt_template_3, output_key=\"language\")\n",
        "\n",
        "prompt_template_4 = ChatPromptTemplate.from_template(\n",
        "    \"Write a follow up response to the following \"\n",
        "    \"summary in the specified language:\"\n",
        "    \"\\n\\nSummary: {summary}\\n\\nLanguage: {language}\"\n",
        ")\n",
        "chain_4 = LLMChain(llm=llm, prompt=prompt_template_4, output_key=\"followup_message\")\n",
        "\n",
        "chain = SequentialChain(\n",
        "    chains=[chain_1, chain_2, chain_3, chain_4],\n",
        "    input_variables=[\"Review\"],\n",
        "    output_variables=[\"English_Review\", \"summary\", \"followup_message\"],\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "chain(review)\n",
        "# `run` not supported when there is not exactly one output key"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pW-zdnRD7fjp",
        "outputId": "c5123158-7a9d-4c2b-9c4f-50fc8261bca5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Review': \"Je trouve le goût médiocre. La mousse ne tient pas, c'est bizarre. J'achète les mêmes dans le commerce et le goût est bien meilleur...\\nVieux lot ou contrefaçon !?\",\n",
              " 'English_Review': \"I find the taste mediocre. The foam doesn't hold, it's weird. I buy the same ones in stores and the taste is much better... Old batch or counterfeit!?\",\n",
              " 'summary': 'The reviewer is dissatisfied with the taste and foam of the product, suspecting it may be an old batch or counterfeit.',\n",
              " 'followup_message': \"Je vous remercie pour votre avis sur notre produit. Nous sommes désolés que vous ayez été insatisfait de son goût et de sa mousse. Nous prenons très au sérieux la qualité de nos produits et nous aimerions en savoir plus sur votre expérience pour investiguer davantage. S'il vous plaît, contactez notre service clientèle pour que nous puissions résoudre ce problème au plus vite. Merci de nous avoir informés.\"}"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "physics_template = \"\"\"You are a very smart physics professor. \\\n",
        "You are great at answering questions about physics in a concise \\\n",
        "and easy to understand manner. \\\n",
        "When you don't know the answer to a question you admit \\\n",
        "that you don't know.\n",
        "\n",
        "Here is a question:\n",
        "{input}\"\"\"\n",
        "\n",
        "math_template = \"\"\"You are a very good mathematician. \\\n",
        "You are great at answering math questions. \\\n",
        "You are so good because you are able to break down \\\n",
        "hard problems into their component parts, \\\n",
        "answer the component parts, and then put them together\\\n",
        "to answer the broader question.\n",
        "\n",
        "Here is a question:\n",
        "{input}\"\"\"\n",
        "\n",
        "history_template = \"\"\"You are a very good historian. \\\n",
        "You have an excellent knowledge of and understanding of people, \\\n",
        "events and contexts from a range of historical periods. \\\n",
        "You have the ability to think, reflect, debate, discuss and \\\n",
        "evaluate the past. You have a respect for historical evidence \\\n",
        "and the ability to make use of it to support your explanations \\\n",
        "and judgements.\n",
        "\n",
        "Here is a question:\n",
        "{input}\"\"\"\n",
        "\n",
        "computerscience_template = \"\"\" You are a successful computer scientist. \\\n",
        "You have a passion for creativity, collaboration, \\\n",
        "forward-thinking, confidence, strong problem-solving capabilities, \\\n",
        "understanding of theories and algorithms, and excellent communication \\\n",
        "skills. You are great at answering coding questions. \\\n",
        "You are so good because you know how to solve a problem by \\\n",
        "describing the solution in imperative steps \\\n",
        "that a machine can easily interpret and you know how to \\\n",
        "choose a solution that has a good balance between \\\n",
        "time complexity and space complexity.\n",
        "\n",
        "Here is a question:\n",
        "{input}\"\"\"\n",
        "\n",
        "prompt_infos = [\n",
        "    {\n",
        "        \"name\": \"physics\",\n",
        "        \"description\": \"Good for answering questions about physics\",\n",
        "        \"prompt_template\": physics_template\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"math\",\n",
        "        \"description\": \"Good for answering math questions\",\n",
        "        \"prompt_template\": math_template\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"History\",\n",
        "        \"description\": \"Good for answering history questions\",\n",
        "        \"prompt_template\": history_template\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"computer science\",\n",
        "        \"description\": \"Good for answering computer science questions\",\n",
        "        \"prompt_template\": computerscience_template\n",
        "    }\n",
        "]\n",
        "\n",
        "# chains that will be called by router chain\n",
        "destination_chains = {}\n",
        "for p_info in prompt_infos:\n",
        "    name = p_info[\"name\"]\n",
        "    prompt_template = p_info[\"prompt_template\"]\n",
        "    prompt = ChatPromptTemplate.from_template(template=prompt_template)\n",
        "    chain = LLMChain(llm=llm, prompt=prompt)\n",
        "    destination_chains[name] = chain\n",
        "\n",
        "destinations = [f\"{p['name']}: {p['description']}\" for p in prompt_infos]\n",
        "destinations_str = \"\\n\".join(destinations)\n",
        "\n",
        "# called when the router can't decide which sub-chain to use\n",
        "default_prompt = ChatPromptTemplate.from_template(\"{input}\")\n",
        "default_chain = LLMChain(llm=llm, prompt=default_prompt)\n",
        "\n",
        "# template that will be used by LLM to route between chains\n",
        "MULTI_PROMPT_ROUTER_TEMPLATE = \"\"\"Given a raw text input to a \\\n",
        "language model select the model prompt best suited for the input. \\\n",
        "You will be given the names of the available prompts and a \\\n",
        "description of what the prompt is best suited for. \\\n",
        "You may also revise the original input if you think that revising \\\n",
        "it will ultimately lead to a better response from the language model.\n",
        "\n",
        "<< FORMATTING >>\n",
        "Return a markdown code snippet with a JSON object formatted to look like:\n",
        "```json\n",
        "{{{{\n",
        "    \"destination\": string \\\\ name of the prompt to use or \"DEFAULT\"\n",
        "    \"next_inputs\": string \\\\ a potentially modified version of the original input\n",
        "}}}}\n",
        "```\n",
        "\n",
        "REMEMBER: \"destination\" MUST be one of the candidate prompt \\\n",
        "names specified below OR it can be \"DEFAULT\" if the input is not \\\n",
        "well suited for any of the candidate prompts.\n",
        "REMEMBER: \"next_inputs\" can just be the original input \\\n",
        "if you don't think any modifications are needed.\n",
        "\n",
        "<< CANDIDATE PROMPTS >>\n",
        "{destinations}\n",
        "\n",
        "<< INPUT >>\n",
        "{{input}}\n",
        "\n",
        "<< OUTPUT (remember to include the ```json)>>\"\"\"\n",
        "\n",
        "router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(destinations=destinations_str)\n",
        "router_prompt = PromptTemplate(template=router_template, input_variables=[\"input\"], output_parser=RouterOutputParser()) # parses the LLM output to determine which chain to use and what the output to that chain should be\n",
        "\n",
        "# for routing between multiple prompt templates\n",
        "router_chain = LLMRouterChain.from_llm(llm, router_prompt)\n",
        "\n",
        "# decides on which sub-chain to route the response as an input (more complex) according to the prompt templates by passing the descriptions to an LLM\n",
        "chain = MultiPromptChain(router_chain=router_chain, destination_chains=destination_chains, default_chain=default_chain, verbose=True)\n",
        "\n",
        "chain.run(\"What is black body radiation?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "id": "NsWgl2isAb89",
        "outputId": "e74eb9eb-5336-417e-f174-7bfc4e4f1cfc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
            "physics: {'input': 'What is black body radiation?'}\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Black body radiation is the electromagnetic radiation emitted by a perfect black body, which absorbs all incoming radiation and emits energy at all wavelengths. The distribution of this radiation follows Planck's law, which describes how the intensity of the radiation changes with temperature. Black body radiation is an important concept in physics and has applications in various fields, including astrophysics and thermodynamics.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question and Answer"
      ],
      "metadata": {
        "id": "LtBAoozGKFhW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Please list all your shirts with sun protection in a table in markdown and summarize each one.\""
      ],
      "metadata": {
        "id": "llfVJDH0gQIh"
      },
      "execution_count": 235,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the documents\n",
        "# because this documents are small, chunking (splitting documents into smaller pieces) isn't necessary\n",
        "loader = CSVLoader(file_path=\"./drive/MyDrive/dataset/OutdoorClothingCatalog_500.csv\", encoding='utf-8')\n",
        "\n",
        "docs = loader.load()\n",
        "docs[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gHweuizuLh10",
        "outputId": "be7bb34b-f5d1-46be-f1df-24858aa2a697"
      },
      "execution_count": 257,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content=\": 0\\nUnnamed: 0: 0\\nname: Women's Campside Oxfords\\ndescription: This ultracomfortable lace-to-toe Oxford boasts a super-soft canvas, thick cushioning, and quality construction for a broken-in feel from the first time you put them on. \\n\\nSize & Fit: Order regular shoe size. For half sizes not offered, order up to next whole size. \\n\\nSpecs: Approx. weight: 1 lb.1 oz. per pair. \\n\\nConstruction: Soft canvas material for a broken-in feel and look. Comfortable EVA innersole with Cleansport NXT® antimicrobial odor control. Vintage hunt, fish and camping motif on innersole. Moderate arch contour of innersole. EVA foam midsole for cushioning and support. Chain-tread-inspired molded rubber outsole with modified chain-tread pattern. Imported. \\n\\nQuestions? Please contact us for any inquiries.\", metadata={'source': './drive/MyDrive/dataset/OutdoorClothingCatalog_500.csv', 'row': 0})"
            ]
          },
          "metadata": {},
          "execution_count": 257
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# numerical representations of pieces of text that captures semantic meaning\n",
        "embeddings = OpenAIEmbeddings()\n",
        "\n",
        "embed = embeddings.embed_query(query)\n",
        "print(len(embed))\n",
        "embed[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QmKclZLpMDrf",
        "outputId": "a7ae9ae1-424c-46b3-8a0d-4e8f05598bba"
      },
      "execution_count": 258,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1536\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.003285329438041333,\n",
              " 0.001319983088892754,\n",
              " 0.023928350673644135,\n",
              " -0.032684640238161373,\n",
              " -0.006897642983824973]"
            ]
          },
          "metadata": {},
          "execution_count": 258
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# basic vector store without needing to connect an external db\n",
        "db = DocArrayInMemorySearch.from_documents(docs, embeddings)\n",
        "\n",
        "# pieces of text with similar contents will have similar embeddings\n",
        "docs = db.similarity_search(query)\n",
        "print(len(docs))\n",
        "docs[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DAxTbHiBM7XU",
        "outputId": "717fe48a-b0d5-4c23-be4f-54993ce00ea2"
      },
      "execution_count": 259,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content=\": 374\\nUnnamed: 0: 374\\nname: Men's Plaid Tropic Shirt, Short-Sleeve\\ndescription: Our Ultracomfortable sun protection is rated to UPF 50+, helping you stay cool and dry. Originally designed for fishing, this lightest hot-weather shirt offers UPF 50+ coverage and is great for extended travel. SunSmart technology blocks 98% of the sun's harmful UV rays, while the high-performance fabric is wrinkle-free and quickly evaporates perspiration. Made with 52% polyester and 48% nylon, this shirt is machine washable and dryable. Additional features include front and back cape venting, two front bellows pockets and an imported design. With UPF 50+ coverage, you can limit sun exposure and feel secure with the highest rated sun protection available.\", metadata={'source': './drive/MyDrive/dataset/OutdoorClothingCatalog_500.csv', 'row': 374})"
            ]
          },
          "metadata": {},
          "execution_count": 259
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm = OpenAI(temperature=0.0, model=\"gpt-3.5-turbo-instruct\")\n",
        "# create vector store\n",
        "index = VectorstoreIndexCreator(vectorstore_cls=DocArrayInMemorySearch, embedding=embeddings).from_loaders([loader])\n",
        "\n",
        "response = index.query(query, llm=llm)\n",
        "display(Markdown(response))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        },
        "id": "A-FINesa_IXH",
        "outputId": "722b15d8-bbec-453e-abec-c6446463a195"
      },
      "execution_count": 268,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n\n| Name | Description | Sun Protection Rating |\n| --- | --- | --- |\n| Men's Plaid Tropic Shirt, Short-Sleeve | Ultracomfortable sun protection rated to UPF 50+. Made with 52% polyester and 48% nylon. Features front and back cape venting, two front bellows pockets. | UPF 50+ |\n| Sun Shield Shirt by | High-performance sun shirt with UPF 50+ rating. Made with 78% nylon and 22% Lycra Xtra Life fiber. Wicks moisture and abrasion resistant. | UPF 50+ |\n| Girls' Ocean Breeze Long-Sleeve Stripe Shirt | Long-sleeve sun-protection rash guard with UPF 50+ rating. Made with Nylon Lycra®-elastane blend. Quick-drying and fade-resistant. | UPF 50+ |\n\nEach of these shirts offers sun protection with a UPF 50+ rating, blocking 98% of the sun's harmful UV rays. They are all made with high-performance fabrics that are quick-drying and recommended by The Skin Cancer Foundation. The Men's Plaid Tropic Shirt and Sun Shield Shirt also have additional features such as venting and pockets"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qdocs = \"\".join([docs[i].page_content for i in range(len(docs))])\n",
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
        "response = llm([HumanMessage(content=f\"{qdocs}\\nQuestion: {query}\")]).content\n",
        "display(Markdown(response))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "ld_yF2EnOe83",
        "outputId": "c4107ff1-b721-45ed-8ac9-ea4c891f8e4e"
      },
      "execution_count": 226,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "| Shirt Name                                      | Summary                                                                                                                                                                                                                                                                          |\n|-----------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Men's Plaid Tropic Shirt, Short-Sleeve        | This shirt offers UPF 50+ sun protection, blocks 98% of harmful UV rays, is wrinkle-free, and quickly evaporates perspiration. Made with 52% polyester and 48% nylon, it features front and back cape venting and two front bellows pockets.                    |\n| Sun Shield Shirt by                           | This high-performance sun shirt provides SPF 50+ sun protection, blocks 98% of harmful rays, and is made of 78% nylon and 22% Lycra Xtra Life fiber. It is quick-drying, abrasion-resistant, and fits comfortably over swimsuits.                                                |\n| Girls' Ocean Breeze Long-Sleeve Stripe Shirt | This long-sleeve rash guard offers full-coverage sun protection with UPF 50+. Made of Nylon Lycra-elastane blend, it is quick-drying, fade-resistant, and seawater-resistant. Recommended by The Skin Cancer Foundation for UV protection.                        |"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm = OpenAI(temperature=0.0, model=\"gpt-3.5-turbo-instruct\")\n",
        "retriever = db.as_retriever() # takes in a query and return fetched documents from the vector store\n",
        "\n",
        "qa_stuff = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type='stuff', # stuffs all the documents into context for a single call to the LLM (cheap but not ideal for large documents)\n",
        "    # other methods:\n",
        "    # -map_reduce: make multiple calls with each retrieved document, then make a final call to summarize the answers (popular even for summarization, supports large and vast amount of documents, supports parallel fast computing, but expensive and can't evaluate all of the information at once)\n",
        "    # -refine: iterativly make multiple calls with each retrieved document by building on the answer (combines information, but expensive and slow because each step depends on the previous one)\n",
        "    # -map_rerank: make multiple calls with each retrieved document and ask for a score from the LLM, then select the most relevant answer with the highest score (experimental, relies on the LLM to know the score, expensive)\n",
        "    retriever=retriever,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "response = qa_stuff.run(query)\n",
        "display(Markdown(response))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "id": "OgC4PV5tNkmx",
        "outputId": "b4122d7a-b0bf-4c75-b242-38cde706bfb7"
      },
      "execution_count": 252,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n\n| Name | Description | Sun Protection Rating |\n| --- | --- | --- |\n| Men's Plaid Tropic Shirt, Short-Sleeve | Made with UPF 50+ coverage, blocks 98% of harmful UV rays, wrinkle-free, quick-drying | UPF 50+ |\n| Sun Shield Shirt by | High-performance fabric with SPF 50+ sun protection, wicks moisture, abrasion resistant | SPF 50+ |\n| Girls' Ocean Breeze Long-Sleeve Stripe Shirt | Made with UPF 50+ coverage, blocks 98% of harmful UV rays, quick-drying, fade-resistant | UPF 50+ |\n| Classic Plaid Short-Sleeve Shirt | Made with pure European flax, lightweight and breathable | N/A |"
          },
          "metadata": {}
        }
      ]
    }
  ]
}
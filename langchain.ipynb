{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "RZtGHoHwraEl"
      ],
      "authorship_tag": "ABX9TyNdx7GqWWib+hmtxSsBEnYB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mertcan-basut/nlp/blob/main/langchain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q openai\n",
        "\n",
        "!pip install -q langchain langchain-openai\n",
        "\n",
        "!pip install -q python-dotenv"
      ],
      "metadata": {
        "id": "wGmNpMwOKna1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!echo \"OPENAI_API_KEY=editthis\" > .env"
      ],
      "metadata": {
        "id": "kZUgLx7JHh6Z"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "CjpO4-lVFA2z"
      },
      "outputs": [],
      "source": [
        "# a framework for developing LM powered applications\n",
        "from langchain_openai import ChatOpenAI # models\n",
        "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, SystemMessagePromptTemplate # input prompts\n",
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "from langchain.output_parsers import StructuredOutputParser # output parsers\n",
        "from langchain.output_parsers import ResponseSchema\n",
        "from langchain.chains import ConversationChain # memory\n",
        "from langchain.memory import ConversationBufferMemory, ConversationBufferWindowMemory, ConversationTokenBufferMemory, ConversationSummaryBufferMemory\n",
        "\n",
        "import openai # direct API calls to OpenAI\n",
        "\n",
        "import json\n",
        "import os\n",
        "# https://platform.openai.com/api-keys\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "_ = load_dotenv(find_dotenv()) # read local .env file"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Models, Prompts, Parsers"
      ],
      "metadata": {
        "id": "RZtGHoHwraEl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"\\\n",
        "This leaf blower is pretty amazing.  It has four settings: \\\n",
        "candle blower, gentle breeze, windy city, and tornado. \\\n",
        "It arrived in two days, just in time for my wife's \\\n",
        "anniversary present. \\\n",
        "I think my wife liked it so much she was speechless. \\\n",
        "So far I've been the only one using it, and I've been \\\n",
        "using it every other morning to clear the leaves on our lawn. \\\n",
        "It's slightly more expensive than the other leaf blowers \\\n",
        "out there, but I think it's worth it for the extra features.\\\n",
        "\"\"\"\n",
        "\n",
        "system_message_template = \"\"\"\\\n",
        "For the following text, extract the following information:\n",
        "\n",
        "gift: Was the item purchased as a gift for someone else? Answer True if yes, False if not or unknown.\n",
        "\n",
        "delivery_days: How many days did it take for the product to arrive? If this information is not found, output -1.\n",
        "\n",
        "price_value: Extract any sentences about the value or price, and output them as a comma separated Python list.\n",
        "\"\"\"\n",
        "\n",
        "human_message_template = \"\"\"\\\n",
        "text: {text}\n",
        "\"\"\"\n",
        "\n",
        "format_instructions_template = \"\"\"\\\n",
        "Format the output as JSON with the following keys:\n",
        "gift\n",
        "delivery_days\n",
        "price_value\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "CuOOgUWy00AI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client = openai.OpenAI(api_key=os.environ['OPENAI_API_KEY'])\n",
        "\n",
        "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        temperature=0,\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "get_completion(\"Hi!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "ez7AjgGLJ7jq",
        "outputId": "00398d69-ff06-48f4-845e-66a396d9113b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hello! How can I assist you today?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def format_prompt(system_message, human_message, format_instructions):\n",
        "  return f\"\"\"\\\n",
        "{system_message}\n",
        "\n",
        "{human_message}\n",
        "\n",
        "{format_instructions}\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "system_message_prompt = system_message_template.format()\n",
        "human_message_prompt = human_message_template.format(text=text)\n",
        "format_instructions_prompt = format_instructions_template.format()\n",
        "prompt = format_prompt(system_message_prompt, human_message_prompt, format_instructions_prompt)\n",
        "print(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "srwP4Vv99JSI",
        "outputId": "ef3c3065-bde7-464f-9ac3-faa85cefee16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For the following text, extract the following information:\n",
            "\n",
            "gift: Was the item purchased as a gift for someone else? Answer True if yes, False if not or unknown.\n",
            "\n",
            "delivery_days: How many days did it take for the product to arrive? If this information is not found, output -1.\n",
            "\n",
            "price_value: Extract any sentences about the value or price, and output them as a comma separated Python list.\n",
            "\n",
            "\n",
            "text: This leaf blower is pretty amazing.  It has four settings: candle blower, gentle breeze, windy city, and tornado. It arrived in two days, just in time for my wife's anniversary present. I think my wife liked it so much she was speechless. So far I've been the only one using it, and I've been using it every other morning to clear the leaves on our lawn. It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\n",
            "\n",
            "\n",
            "Format the output as JSON with the following keys:\n",
            "gift\n",
            "delivery_days\n",
            "price_value\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = get_completion(prompt)\n",
        "print(response)\n",
        "\n",
        "output_dict = json.loads(response)\n",
        "output_dict.get('delivery_days')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YlXK5Ub0BNOD",
        "outputId": "9e7dc2ef-6603-45a6-fd90-de7de3897ed0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"gift\": true,\n",
            "  \"delivery_days\": 2,\n",
            "  \"price_value\": [\"It's slightly more expensive than the other leaf blowers out there\"]\n",
            "}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 136
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.0)\n",
        "\n",
        "chat([HumanMessage(content=\"Hi!\")]).content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "qlSD8PcbwgSN",
        "outputId": "a73b6afd-0cc9-457e-e3f1-39506162dc02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hello! How can I assist you today?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "system_message_prompt = SystemMessagePromptTemplate.from_template(system_message_template)\n",
        "human_message_prompt = HumanMessagePromptTemplate.from_template(human_message_template)\n",
        "\n",
        "gift_schema = ResponseSchema(name=\"gift\", description=\"Was the item purchased as a gift for someone else? Answer True if yes, False if not or unknown.\")\n",
        "delivery_days_schema = ResponseSchema(name=\"delivery_days\", description=\"How many days did it take for the product to arrive? If this information is not found, output -1.\")\n",
        "price_value_schema = ResponseSchema(name=\"price_value\", description=\"Extract any sentences about the value or price, and output them as a comma separated Python list.\")\n",
        "response_schemas = [gift_schema, delivery_days_schema, price_value_schema]\n",
        "\n",
        "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
        "\n",
        "format_instructions = output_parser.get_format_instructions()\n",
        "format_instructions_prompt = SystemMessage(content=format_instructions)\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt, format_instructions_prompt])\n",
        "prompt = prompt_template.format_messages(text=text)\n",
        "for message in prompt: print(message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-m-vJjDA5ZA",
        "outputId": "aa155e13-0b14-4294-aa19-7c788e054f82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For the following text, extract the following information:\n",
            "\n",
            "gift: Was the item purchased as a gift for someone else? Answer True if yes, False if not or unknown.\n",
            "\n",
            "delivery_days: How many days did it take for the product to arrive? If this information is not found, output -1.\n",
            "\n",
            "price_value: Extract any sentences about the value or price, and output them as a comma separated Python list.\n",
            "\n",
            "text: This leaf blower is pretty amazing.  It has four settings: candle blower, gentle breeze, windy city, and tornado. It arrived in two days, just in time for my wife's anniversary present. I think my wife liked it so much she was speechless. So far I've been the only one using it, and I've been using it every other morning to clear the leaves on our lawn. It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\n",
            "\n",
            "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
            "\n",
            "```json\n",
            "{\n",
            "\t\"gift\": string  // Was the item purchased as a gift for someone else? Answer True if yes, False if not or unknown.\n",
            "\t\"delivery_days\": string  // How many days did it take for the product to arrive? If this information is not found, output -1.\n",
            "\t\"price_value\": string  // Extract any sentences about the value or price, and output them as a comma separated Python list.\n",
            "}\n",
            "```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = chat(prompt).content\n",
        "print(response)\n",
        "\n",
        "output_dict = output_parser.parse(response)\n",
        "output_dict.get('delivery_days')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3gB1eU40xo2",
        "outputId": "6c69f3cd-fd26-4871-9a99-f01b35a2ad7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "```json\n",
            "{\n",
            "\t\"gift\": true,\n",
            "\t\"delivery_days\": 2,\n",
            "\t\"price_value\": \"It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\"\n",
            "}\n",
            "```\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Memory"
      ],
      "metadata": {
        "id": "DIGo1r1jsk49"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(temperature=0.0, model=\"gpt-3.5-turbo\")\n",
        "memory = ConversationBufferMemory()\n",
        "conversation = ConversationChain(llm=llm, memory = memory, verbose=True)"
      ],
      "metadata": {
        "id": "OwVMxqjmsmnm"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(conversation.predict(input=\"Hi, my name is Mert.\"))\n",
        "print(conversation.predict(input=\"What is 1+1?\"))\n",
        "print(conversation.predict(input=\"What is my name?\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MWPx4j9UtNcL",
        "outputId": "6317b33a-6c51-46b6-c90b-e8abc9407603"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: Hi, my name is Mert.\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Hello Mert! It's nice to meet you. How can I assist you today?\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Hi, my name is Mert.\n",
            "AI: Hello Mert! It's nice to meet you. How can I assist you today?\n",
            "Human: What is 1+1?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "1 + 1 equals 2. Is there anything else you would like to know?\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Hi, my name is Mert.\n",
            "AI: Hello Mert! It's nice to meet you. How can I assist you today?\n",
            "Human: What is 1+1?\n",
            "AI: 1 + 1 equals 2. Is there anything else you would like to know?\n",
            "Human: What is my name?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Your name is Mert. Is there anything else you would like to know or discuss?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(memory.buffer, '\\n')\n",
        "memory.load_memory_variables({})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZybXyJK4t6Yl",
        "outputId": "e9de7485-0da6-4733-b583-2da42c9e7095"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Human: Hi, my name is Mert.\n",
            "AI: Hello Mert! It's nice to meet you. How can I assist you today?\n",
            "Human: What is 1+1?\n",
            "AI: 1 + 1 equals 2. Is there anything else you would like to know?\n",
            "Human: What is my name?\n",
            "AI: Your name is Mert. Is there anything else you would like to know or discuss? \n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': \"Human: Hi, my name is Mert.\\nAI: Hello Mert! It's nice to meet you. How can I assist you today?\\nHuman: What is 1+1?\\nAI: 1 + 1 equals 2. Is there anything else you would like to know?\\nHuman: What is my name?\\nAI: Your name is Mert. Is there anything else you would like to know or discuss?\"}"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# add aditional data to memory\n",
        "memory.save_context({\"input\": \"Hi!\"}, {\"output\": \"What's up?\"})\n",
        "print(memory.buffer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uTgquHyKukqJ",
        "outputId": "522db347-d635-4ee9-d703-e03a3e716e1d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Human: Hi, my name is Mert.\n",
            "AI: Hello Mert! It's nice to meet you. How can I assist you today?\n",
            "Human: What is 1+1?\n",
            "AI: 1 + 1 equals 2. Is there anything else you would like to know?\n",
            "Human: What is my name?\n",
            "AI: Your name is Mert. Is there anything else you would like to know or discuss?\n",
            "Human: Hi!\n",
            "AI: What's up?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "memory = ConversationBufferWindowMemory(k=1) # only keeps most recent `k` conversations\n",
        "\n",
        "memory.save_context({\"input\": \"Hi!\"}, {\"output\": \"What's up?\"})\n",
        "memory.save_context({\"input\": \"Not much, just hanging.\"}, {\"output\": \"Cool.\"})\n",
        "\n",
        "print(memory.buffer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gOdw6gLHv8Nf",
        "outputId": "a672f9aa-7de3-45d2-8273-21feeda5f982"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Human: Not much, just hanging.\n",
            "AI: Cool.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=30) # chops off the earlier parts of the conversation to not exceed the token limit dependent on the LLM because usually cost is determined by number of tokens\n",
        "\n",
        "memory.save_context({\"input\": \"AI is what?!\"}, {\"output\": \"Amazing!\"})\n",
        "memory.save_context({\"input\": \"Backpropagation is what?\"}, {\"output\": \"Beautiful!\"})\n",
        "memory.save_context({\"input\": \"Chatbots are what?\"}, {\"output\": \"Charming!\"})\n",
        "\n",
        "print(memory.buffer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OZJ_TWC_w8v3",
        "outputId": "67c8c412-abca-4aa6-9242-36fe2e280fac"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AI: Beautiful!\n",
            "Human: Chatbots are what?\n",
            "AI: Charming!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=30) # uses the specified LLM to summarize the entire chat history which exceeds the specified number of tokens\n",
        "\n",
        "memory.save_context({\"input\": \"AI is what?!\"}, {\"output\": \"Amazing!\"})\n",
        "memory.save_context({\"input\": \"Backpropagation is what?\"}, {\"output\": \"Beautiful!\"})\n",
        "memory.save_context({\"input\": \"Chatbots are what?\"}, {\"output\": \"Charming!\"})\n",
        "\n",
        "print(memory.load_memory_variables({})['history'])\n",
        "# `System` is not official OpenAI system message!"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aspHswv2x1IW",
        "outputId": "4a77020d-74bb-44f9-f34d-d971dfa27620"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "System: The human expresses surprise at the AI's positive view of artificial intelligence. The AI responds with \"Amazing!\" and the human asks about backpropagation.\n",
            "AI: Beautiful!\n",
            "Human: Chatbots are what?\n",
            "AI: Charming!\n"
          ]
        }
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "RZtGHoHwraEl",
        "DIGo1r1jsk49"
      ],
      "authorship_tag": "ABX9TyMqV9f6N0Ix5kHirvJsT5v2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mertcan-basut/nlp/blob/main/langchain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q openai\n",
        "\n",
        "!pip install -q langchain langchain-openai\n",
        "\n",
        "!pip install -q python-dotenv"
      ],
      "metadata": {
        "id": "wGmNpMwOKna1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!echo \"OPENAI_API_KEY=editthis\" > .env"
      ],
      "metadata": {
        "id": "kZUgLx7JHh6Z"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "CjpO4-lVFA2z"
      },
      "outputs": [],
      "source": [
        "# a framework for developing LM powered applications\n",
        "from langchain_openai import ChatOpenAI # models\n",
        "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, SystemMessagePromptTemplate, PromptTemplate # input prompts\n",
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "from langchain.output_parsers import StructuredOutputParser # output parsers\n",
        "from langchain.output_parsers import ResponseSchema\n",
        "from langchain.chains import ConversationChain # memory\n",
        "from langchain.memory import ConversationBufferMemory, ConversationBufferWindowMemory, ConversationTokenBufferMemory, ConversationSummaryBufferMemory\n",
        "from langchain.chains import LLMChain, SimpleSequentialChain, SequentialChain # chains\n",
        "from langchain.chains.router import MultiPromptChain\n",
        "from langchain.chains.router.llm_router import LLMRouterChain, RouterOutputParser\n",
        "\n",
        "import openai # direct API calls to OpenAI\n",
        "\n",
        "import json\n",
        "import os\n",
        "# https://platform.openai.com/api-keys\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "_ = load_dotenv(find_dotenv()) # read local .env file\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "# /usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Models, Prompts, Parsers"
      ],
      "metadata": {
        "id": "RZtGHoHwraEl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"\\\n",
        "This leaf blower is pretty amazing.  It has four settings: \\\n",
        "candle blower, gentle breeze, windy city, and tornado. \\\n",
        "It arrived in two days, just in time for my wife's \\\n",
        "anniversary present. \\\n",
        "I think my wife liked it so much she was speechless. \\\n",
        "So far I've been the only one using it, and I've been \\\n",
        "using it every other morning to clear the leaves on our lawn. \\\n",
        "It's slightly more expensive than the other leaf blowers \\\n",
        "out there, but I think it's worth it for the extra features.\\\n",
        "\"\"\"\n",
        "\n",
        "system_message_template = \"\"\"\\\n",
        "For the following text, extract the following information:\n",
        "\n",
        "gift: Was the item purchased as a gift for someone else? Answer True if yes, False if not or unknown.\n",
        "\n",
        "delivery_days: How many days did it take for the product to arrive? If this information is not found, output -1.\n",
        "\n",
        "price_value: Extract any sentences about the value or price, and output them as a comma separated Python list.\n",
        "\"\"\"\n",
        "\n",
        "human_message_template = \"\"\"\\\n",
        "text: {text}\n",
        "\"\"\"\n",
        "\n",
        "format_instructions_template = \"\"\"\\\n",
        "Format the output as JSON with the following keys:\n",
        "gift\n",
        "delivery_days\n",
        "price_value\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "CuOOgUWy00AI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client = openai.OpenAI(api_key=os.environ['OPENAI_API_KEY'])\n",
        "\n",
        "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        temperature=0,\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "get_completion(\"Hi!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "ez7AjgGLJ7jq",
        "outputId": "00398d69-ff06-48f4-845e-66a396d9113b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hello! How can I assist you today?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def format_prompt(system_message, human_message, format_instructions):\n",
        "  return f\"\"\"\\\n",
        "{system_message}\n",
        "\n",
        "{human_message}\n",
        "\n",
        "{format_instructions}\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "system_message_prompt = system_message_template.format()\n",
        "human_message_prompt = human_message_template.format(text=text)\n",
        "format_instructions_prompt = format_instructions_template.format()\n",
        "prompt = format_prompt(system_message_prompt, human_message_prompt, format_instructions_prompt)\n",
        "print(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "srwP4Vv99JSI",
        "outputId": "ef3c3065-bde7-464f-9ac3-faa85cefee16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For the following text, extract the following information:\n",
            "\n",
            "gift: Was the item purchased as a gift for someone else? Answer True if yes, False if not or unknown.\n",
            "\n",
            "delivery_days: How many days did it take for the product to arrive? If this information is not found, output -1.\n",
            "\n",
            "price_value: Extract any sentences about the value or price, and output them as a comma separated Python list.\n",
            "\n",
            "\n",
            "text: This leaf blower is pretty amazing.  It has four settings: candle blower, gentle breeze, windy city, and tornado. It arrived in two days, just in time for my wife's anniversary present. I think my wife liked it so much she was speechless. So far I've been the only one using it, and I've been using it every other morning to clear the leaves on our lawn. It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\n",
            "\n",
            "\n",
            "Format the output as JSON with the following keys:\n",
            "gift\n",
            "delivery_days\n",
            "price_value\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = get_completion(prompt)\n",
        "print(response)\n",
        "\n",
        "output_dict = json.loads(response)\n",
        "output_dict.get('delivery_days')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YlXK5Ub0BNOD",
        "outputId": "9e7dc2ef-6603-45a6-fd90-de7de3897ed0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"gift\": true,\n",
            "  \"delivery_days\": 2,\n",
            "  \"price_value\": [\"It's slightly more expensive than the other leaf blowers out there\"]\n",
            "}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 136
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.0)\n",
        "\n",
        "chat([HumanMessage(content=\"Hi!\")]).content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "qlSD8PcbwgSN",
        "outputId": "a73b6afd-0cc9-457e-e3f1-39506162dc02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hello! How can I assist you today?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "system_message_prompt = SystemMessagePromptTemplate.from_template(system_message_template)\n",
        "human_message_prompt = HumanMessagePromptTemplate.from_template(human_message_template)\n",
        "\n",
        "gift_schema = ResponseSchema(name=\"gift\", description=\"Was the item purchased as a gift for someone else? Answer True if yes, False if not or unknown.\")\n",
        "delivery_days_schema = ResponseSchema(name=\"delivery_days\", description=\"How many days did it take for the product to arrive? If this information is not found, output -1.\")\n",
        "price_value_schema = ResponseSchema(name=\"price_value\", description=\"Extract any sentences about the value or price, and output them as a comma separated Python list.\")\n",
        "response_schemas = [gift_schema, delivery_days_schema, price_value_schema]\n",
        "\n",
        "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
        "\n",
        "format_instructions = output_parser.get_format_instructions()\n",
        "format_instructions_prompt = SystemMessage(content=format_instructions)\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt, format_instructions_prompt])\n",
        "prompt = prompt_template.format_messages(text=text)\n",
        "for message in prompt: print(message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-m-vJjDA5ZA",
        "outputId": "aa155e13-0b14-4294-aa19-7c788e054f82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For the following text, extract the following information:\n",
            "\n",
            "gift: Was the item purchased as a gift for someone else? Answer True if yes, False if not or unknown.\n",
            "\n",
            "delivery_days: How many days did it take for the product to arrive? If this information is not found, output -1.\n",
            "\n",
            "price_value: Extract any sentences about the value or price, and output them as a comma separated Python list.\n",
            "\n",
            "text: This leaf blower is pretty amazing.  It has four settings: candle blower, gentle breeze, windy city, and tornado. It arrived in two days, just in time for my wife's anniversary present. I think my wife liked it so much she was speechless. So far I've been the only one using it, and I've been using it every other morning to clear the leaves on our lawn. It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\n",
            "\n",
            "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
            "\n",
            "```json\n",
            "{\n",
            "\t\"gift\": string  // Was the item purchased as a gift for someone else? Answer True if yes, False if not or unknown.\n",
            "\t\"delivery_days\": string  // How many days did it take for the product to arrive? If this information is not found, output -1.\n",
            "\t\"price_value\": string  // Extract any sentences about the value or price, and output them as a comma separated Python list.\n",
            "}\n",
            "```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = chat(prompt).content\n",
        "print(response)\n",
        "\n",
        "output_dict = output_parser.parse(response)\n",
        "output_dict.get('delivery_days')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3gB1eU40xo2",
        "outputId": "6c69f3cd-fd26-4871-9a99-f01b35a2ad7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "```json\n",
            "{\n",
            "\t\"gift\": true,\n",
            "\t\"delivery_days\": 2,\n",
            "\t\"price_value\": \"It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\"\n",
            "}\n",
            "```\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Memory"
      ],
      "metadata": {
        "id": "DIGo1r1jsk49"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(temperature=0.0, model=\"gpt-3.5-turbo\")\n",
        "memory = ConversationBufferMemory()\n",
        "conversation = ConversationChain(llm=llm, memory = memory, verbose=True)"
      ],
      "metadata": {
        "id": "OwVMxqjmsmnm"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(conversation.predict(input=\"Hi, my name is Mert.\"))\n",
        "print(conversation.predict(input=\"What is 1+1?\"))\n",
        "print(conversation.predict(input=\"What is my name?\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MWPx4j9UtNcL",
        "outputId": "29383f61-b9bd-4c75-9423-a9ac1363a38c"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: Hi, my name is Mert.\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Hello Mert! It's nice to meet you. How can I assist you today?\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Hi, my name is Mert.\n",
            "AI: Hello Mert! It's nice to meet you. How can I assist you today?\n",
            "Human: What is 1+1?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "1+1 equals 2. Is there anything else you would like to know?\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Hi, my name is Mert.\n",
            "AI: Hello Mert! It's nice to meet you. How can I assist you today?\n",
            "Human: What is 1+1?\n",
            "AI: 1+1 equals 2. Is there anything else you would like to know?\n",
            "Human: What is my name?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Your name is Mert. Is there anything else you would like to know or discuss?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(memory.buffer, '\\n')\n",
        "memory.load_memory_variables({})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZybXyJK4t6Yl",
        "outputId": "e9de7485-0da6-4733-b583-2da42c9e7095"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Human: Hi, my name is Mert.\n",
            "AI: Hello Mert! It's nice to meet you. How can I assist you today?\n",
            "Human: What is 1+1?\n",
            "AI: 1 + 1 equals 2. Is there anything else you would like to know?\n",
            "Human: What is my name?\n",
            "AI: Your name is Mert. Is there anything else you would like to know or discuss? \n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': \"Human: Hi, my name is Mert.\\nAI: Hello Mert! It's nice to meet you. How can I assist you today?\\nHuman: What is 1+1?\\nAI: 1 + 1 equals 2. Is there anything else you would like to know?\\nHuman: What is my name?\\nAI: Your name is Mert. Is there anything else you would like to know or discuss?\"}"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# add aditional data to memory\n",
        "memory.save_context({\"input\": \"Hi!\"}, {\"output\": \"What's up?\"})\n",
        "print(memory.buffer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uTgquHyKukqJ",
        "outputId": "522db347-d635-4ee9-d703-e03a3e716e1d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Human: Hi, my name is Mert.\n",
            "AI: Hello Mert! It's nice to meet you. How can I assist you today?\n",
            "Human: What is 1+1?\n",
            "AI: 1 + 1 equals 2. Is there anything else you would like to know?\n",
            "Human: What is my name?\n",
            "AI: Your name is Mert. Is there anything else you would like to know or discuss?\n",
            "Human: Hi!\n",
            "AI: What's up?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "memory = ConversationBufferWindowMemory(k=1) # only keeps most recent `k` conversations\n",
        "\n",
        "memory.save_context({\"input\": \"Hi!\"}, {\"output\": \"What's up?\"})\n",
        "memory.save_context({\"input\": \"Not much, just hanging.\"}, {\"output\": \"Cool.\"})\n",
        "\n",
        "print(memory.buffer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gOdw6gLHv8Nf",
        "outputId": "a672f9aa-7de3-45d2-8273-21feeda5f982"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Human: Not much, just hanging.\n",
            "AI: Cool.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=30) # chops off the earlier parts of the conversation to not exceed the token limit dependent on the LLM because usually cost is determined by number of tokens\n",
        "\n",
        "memory.save_context({\"input\": \"AI is what?!\"}, {\"output\": \"Amazing!\"})\n",
        "memory.save_context({\"input\": \"Backpropagation is what?\"}, {\"output\": \"Beautiful!\"})\n",
        "memory.save_context({\"input\": \"Chatbots are what?\"}, {\"output\": \"Charming!\"})\n",
        "\n",
        "print(memory.buffer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OZJ_TWC_w8v3",
        "outputId": "67c8c412-abca-4aa6-9242-36fe2e280fac"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AI: Beautiful!\n",
            "Human: Chatbots are what?\n",
            "AI: Charming!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=30) # uses the specified LLM to summarize the entire chat history which exceeds the specified number of tokens\n",
        "\n",
        "memory.save_context({\"input\": \"AI is what?!\"}, {\"output\": \"Amazing!\"})\n",
        "memory.save_context({\"input\": \"Backpropagation is what?\"}, {\"output\": \"Beautiful!\"})\n",
        "memory.save_context({\"input\": \"Chatbots are what?\"}, {\"output\": \"Charming!\"})\n",
        "\n",
        "print(memory.load_memory_variables({})['history'])\n",
        "# `System` is not official OpenAI system message!"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aspHswv2x1IW",
        "outputId": "4a77020d-74bb-44f9-f34d-d971dfa27620"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "System: The human expresses surprise at the AI's positive view of artificial intelligence. The AI responds with \"Amazing!\" and the human asks about backpropagation.\n",
            "AI: Beautiful!\n",
            "Human: Chatbots are what?\n",
            "AI: Charming!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chains"
      ],
      "metadata": {
        "id": "alVvbSct2NvI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(temperature=0.9, model=\"gpt-3.5-turbo\")\n",
        "\n",
        "product = \"Queen Size Sheet Set\"\n",
        "review = \"Je trouve le goût médiocre. La mousse ne tient pas, c'est bizarre. J'achète les mêmes dans le commerce et le goût est bien meilleur...\\nVieux lot ou contrefaçon !?\""
      ],
      "metadata": {
        "id": "8plS0FvW5lnP"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = ChatPromptTemplate.from_template(\"What is the best name to describe a company that makes {product}?\")\n",
        "chain = LLMChain(llm=llm, prompt=prompt_template) # LLM + prompt (most basic)\n",
        "\n",
        "chain.run(product)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "QhHkL91v2PCG",
        "outputId": "e1477dda-1c57-4e0d-916e-de3870048e55"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Royal Comfort Bedding'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ideal when expecting 1 input & returning 1 output for every chain in sequence (output of the previous chain passed as input into the next chain)\n",
        "prompt_template_1 = ChatPromptTemplate.from_template(\"What is the best name to describe a company that makes {product}?\")\n",
        "chain_1 = LLMChain(llm=llm, prompt=prompt_template_1)\n",
        "\n",
        "prompt_template_2 = ChatPromptTemplate.from_template(\"Write a 20 words description for the following company:{company_name}\")\n",
        "chain_2 = LLMChain(llm=llm, prompt=prompt_template_2)\n",
        "\n",
        "chain = SimpleSequentialChain(chains=[chain_1, chain_2], verbose=True)\n",
        "\n",
        "chain.run(product)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "id": "HIB9ooML5JeZ",
        "outputId": "e44f2ace-3142-4fd7-c32f-08a5cf658fd5"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
            "\u001b[36;1m\u001b[1;3mRegal Dreams Co.\u001b[0m\n",
            "\u001b[33;1m\u001b[1;3mRegal Dreams Co. specializes in luxury home decor items, offering elegant and timeless pieces to elevate any living space.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Regal Dreams Co. specializes in luxury home decor items, offering elegant and timeless pieces to elevate any living space.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sequentially executed multiple inputs & multiple outputs (more complicated)\n",
        "prompt_template_1 = ChatPromptTemplate.from_template(\"Translate the following review to english:\\n\\n{Review}\")\n",
        "chain_1 = LLMChain(llm=llm, prompt=prompt_template_1, output_key=\"English_Review\")\n",
        "\n",
        "prompt_template_2 = ChatPromptTemplate.from_template(\"Can you summarize the following review in 1 sentence:\\n\\n{English_Review}\")\n",
        "chain_2 = LLMChain(llm=llm, prompt=prompt_template_2, output_key=\"summary\")\n",
        "\n",
        "prompt_template_3 = ChatPromptTemplate.from_template(\"What language is the following review:\\n\\n{Review}\")\n",
        "chain_3 = LLMChain(llm=llm, prompt=prompt_template_3, output_key=\"language\")\n",
        "\n",
        "prompt_template_4 = ChatPromptTemplate.from_template(\n",
        "    \"Write a follow up response to the following \"\n",
        "    \"summary in the specified language:\"\n",
        "    \"\\n\\nSummary: {summary}\\n\\nLanguage: {language}\"\n",
        ")\n",
        "chain_4 = LLMChain(llm=llm, prompt=prompt_template_4, output_key=\"followup_message\")\n",
        "\n",
        "chain = SequentialChain(\n",
        "    chains=[chain_1, chain_2, chain_3, chain_4],\n",
        "    input_variables=[\"Review\"],\n",
        "    output_variables=[\"English_Review\", \"summary\", \"followup_message\"],\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "chain(review)\n",
        "# `run` not supported when there is not exactly one output key"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pW-zdnRD7fjp",
        "outputId": "c5123158-7a9d-4c2b-9c4f-50fc8261bca5"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Review': \"Je trouve le goût médiocre. La mousse ne tient pas, c'est bizarre. J'achète les mêmes dans le commerce et le goût est bien meilleur...\\nVieux lot ou contrefaçon !?\",\n",
              " 'English_Review': \"I find the taste mediocre. The foam doesn't hold, it's weird. I buy the same ones in stores and the taste is much better... Old batch or counterfeit!?\",\n",
              " 'summary': 'The reviewer is dissatisfied with the taste and foam of the product, suspecting it may be an old batch or counterfeit.',\n",
              " 'followup_message': \"Je vous remercie pour votre avis sur notre produit. Nous sommes désolés que vous ayez été insatisfait de son goût et de sa mousse. Nous prenons très au sérieux la qualité de nos produits et nous aimerions en savoir plus sur votre expérience pour investiguer davantage. S'il vous plaît, contactez notre service clientèle pour que nous puissions résoudre ce problème au plus vite. Merci de nous avoir informés.\"}"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "physics_template = \"\"\"You are a very smart physics professor. \\\n",
        "You are great at answering questions about physics in a concise \\\n",
        "and easy to understand manner. \\\n",
        "When you don't know the answer to a question you admit \\\n",
        "that you don't know.\n",
        "\n",
        "Here is a question:\n",
        "{input}\"\"\"\n",
        "\n",
        "math_template = \"\"\"You are a very good mathematician. \\\n",
        "You are great at answering math questions. \\\n",
        "You are so good because you are able to break down \\\n",
        "hard problems into their component parts, \\\n",
        "answer the component parts, and then put them together\\\n",
        "to answer the broader question.\n",
        "\n",
        "Here is a question:\n",
        "{input}\"\"\"\n",
        "\n",
        "history_template = \"\"\"You are a very good historian. \\\n",
        "You have an excellent knowledge of and understanding of people, \\\n",
        "events and contexts from a range of historical periods. \\\n",
        "You have the ability to think, reflect, debate, discuss and \\\n",
        "evaluate the past. You have a respect for historical evidence \\\n",
        "and the ability to make use of it to support your explanations \\\n",
        "and judgements.\n",
        "\n",
        "Here is a question:\n",
        "{input}\"\"\"\n",
        "\n",
        "computerscience_template = \"\"\" You are a successful computer scientist. \\\n",
        "You have a passion for creativity, collaboration, \\\n",
        "forward-thinking, confidence, strong problem-solving capabilities, \\\n",
        "understanding of theories and algorithms, and excellent communication \\\n",
        "skills. You are great at answering coding questions. \\\n",
        "You are so good because you know how to solve a problem by \\\n",
        "describing the solution in imperative steps \\\n",
        "that a machine can easily interpret and you know how to \\\n",
        "choose a solution that has a good balance between \\\n",
        "time complexity and space complexity.\n",
        "\n",
        "Here is a question:\n",
        "{input}\"\"\"\n",
        "\n",
        "prompt_infos = [\n",
        "    {\n",
        "        \"name\": \"physics\",\n",
        "        \"description\": \"Good for answering questions about physics\",\n",
        "        \"prompt_template\": physics_template\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"math\",\n",
        "        \"description\": \"Good for answering math questions\",\n",
        "        \"prompt_template\": math_template\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"History\",\n",
        "        \"description\": \"Good for answering history questions\",\n",
        "        \"prompt_template\": history_template\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"computer science\",\n",
        "        \"description\": \"Good for answering computer science questions\",\n",
        "        \"prompt_template\": computerscience_template\n",
        "    }\n",
        "]\n",
        "\n",
        "# chains that will be called by router chain\n",
        "destination_chains = {}\n",
        "for p_info in prompt_infos:\n",
        "    name = p_info[\"name\"]\n",
        "    prompt_template = p_info[\"prompt_template\"]\n",
        "    prompt = ChatPromptTemplate.from_template(template=prompt_template)\n",
        "    chain = LLMChain(llm=llm, prompt=prompt)\n",
        "    destination_chains[name] = chain\n",
        "\n",
        "destinations = [f\"{p['name']}: {p['description']}\" for p in prompt_infos]\n",
        "destinations_str = \"\\n\".join(destinations)\n",
        "\n",
        "# called when the router can't decide which sub-chain to use\n",
        "default_prompt = ChatPromptTemplate.from_template(\"{input}\")\n",
        "default_chain = LLMChain(llm=llm, prompt=default_prompt)\n",
        "\n",
        "# template that will be used by LLM to route between chains\n",
        "MULTI_PROMPT_ROUTER_TEMPLATE = \"\"\"Given a raw text input to a \\\n",
        "language model select the model prompt best suited for the input. \\\n",
        "You will be given the names of the available prompts and a \\\n",
        "description of what the prompt is best suited for. \\\n",
        "You may also revise the original input if you think that revising \\\n",
        "it will ultimately lead to a better response from the language model.\n",
        "\n",
        "<< FORMATTING >>\n",
        "Return a markdown code snippet with a JSON object formatted to look like:\n",
        "```json\n",
        "{{{{\n",
        "    \"destination\": string \\\\ name of the prompt to use or \"DEFAULT\"\n",
        "    \"next_inputs\": string \\\\ a potentially modified version of the original input\n",
        "}}}}\n",
        "```\n",
        "\n",
        "REMEMBER: \"destination\" MUST be one of the candidate prompt \\\n",
        "names specified below OR it can be \"DEFAULT\" if the input is not \\\n",
        "well suited for any of the candidate prompts.\n",
        "REMEMBER: \"next_inputs\" can just be the original input \\\n",
        "if you don't think any modifications are needed.\n",
        "\n",
        "<< CANDIDATE PROMPTS >>\n",
        "{destinations}\n",
        "\n",
        "<< INPUT >>\n",
        "{{input}}\n",
        "\n",
        "<< OUTPUT (remember to include the ```json)>>\"\"\"\n",
        "\n",
        "router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(destinations=destinations_str)\n",
        "router_prompt = PromptTemplate(template=router_template, input_variables=[\"input\"], output_parser=RouterOutputParser()) # parses the LLM output to determine which chain to use and what the output to that chain should be\n",
        "\n",
        "# for routing between multiple prompt templates\n",
        "router_chain = LLMRouterChain.from_llm(llm, router_prompt)\n",
        "\n",
        "# decides on which sub-chain to route the response as an input (more complex) according to the prompt templates by passing the descriptions to an LLM\n",
        "chain = MultiPromptChain(router_chain=router_chain, destination_chains=destination_chains, default_chain=default_chain, verbose=True)\n",
        "\n",
        "chain.run(\"What is black body radiation?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "id": "NsWgl2isAb89",
        "outputId": "e74eb9eb-5336-417e-f174-7bfc4e4f1cfc"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
            "physics: {'input': 'What is black body radiation?'}\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Black body radiation is the electromagnetic radiation emitted by a perfect black body, which absorbs all incoming radiation and emits energy at all wavelengths. The distribution of this radiation follows Planck's law, which describes how the intensity of the radiation changes with temperature. Black body radiation is an important concept in physics and has applications in various fields, including astrophysics and thermodynamics.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    }
  ]
}
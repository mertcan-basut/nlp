{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "N1YVOZkMsSs3"
      ],
      "authorship_tag": "ABX9TyMy1HcGroosP5tieK7Ew0cL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mertcan-basut/nlp/blob/main/retrieve_and_rerank.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Background Information"
      ],
      "metadata": {
        "id": "N1YVOZkMsSs3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Context recall\n",
        "\n",
        "**recall** *(retrieval evaluation metric)* : How many of the relevant documents are retrieved.\n",
        "\n",
        "`recall@K= # of relevant docs returned / # of relevant documents in dataset`\n",
        "\n",
        "### LLM recall\n",
        "\n",
        "![LLM recall](https://www.pinecone.io/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2Fca206b6ada9163bffad313e0e18feee0b460c768-1212x688.png&w=1920&q=75)\n",
        "\n",
        "**LLM recall** refers to the ability of an LLM to find information from the text placed within its context window.\n",
        "\n",
        "When storing information in the middle of a context window, an LLM's ability to recall that information becomes worse than had it not been provided in the first place.\n",
        "\n",
        "### Two-stage retrieval\n",
        "\n",
        "A **reranking model (cross-encoder)** is a type of model that, given a query and document pair, will output a similarity score. Rerankers are much more accurate than embedding models (bi-encoder). But they are slow, so that is why two-stage retrieval is required to perform reranking on a small set of documents retrieved from a large set.\n",
        "\n",
        "![reranker/cross-encoder](https://www.pinecone.io/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F9f0d2f75571bb58eecf2520a23d300a5fc5b1e2c-2440x1100.png&w=3840&q=75)\n",
        "\n",
        "A reranker can receive the raw information directly into the large transformer computation, meaning less information loss. Rerankers run at user query time, and this allows analyzing the document's meaning specific to the user query.\n",
        "\n",
        "![embedding model/bi-encoder](https://www.pinecone.io/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F4509817116ab72e27bae809c38cb48fbf1578b5d-2760x1420.png&w=3840&q=75)\n",
        "\n",
        "Bi-encoders must compress all of the possible meanings of a document into a single vector resulting in information loss. Additionally, bi-encoders have no context on the query because the embeddings are created before user query time.\n",
        "\n",
        "### Sources\n",
        "ðŸŒhttps://www.pinecone.io/learn/series/rag/rerankers/"
      ],
      "metadata": {
        "id": "dueV0d6Zimhl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementation"
      ],
      "metadata": {
        "id": "vymIHTEuskdi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchain langchain-openai langchain-community\n",
        "!pip install -q chromadb\n",
        "!pip install -q python-dotenv"
      ],
      "metadata": {
        "id": "g_gorDLj7k5X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!echo \"AZURE_OPENAI_API_KEY=editme\" > .env\n",
        "!echo \"AZURE_OPENAI_ENDPOINT=editme\" >> .env"
      ],
      "metadata": {
        "id": "zrbRzf1p6lZy"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai.embeddings import AzureOpenAIEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.docstore.document import Document as LangChainDocument\n",
        "\n",
        "import json\n",
        "\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "_ = load_dotenv(find_dotenv(), override=True) # read local .env file\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CtoMTd9BtWC9",
        "outputId": "1aacbf48-1334-499c-d666-e36b99d4ec68"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare data and vector store"
      ],
      "metadata": {
        "id": "l4YXlFvttM45"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/drive/MyDrive/data/corpus_dataset.json\", 'r') as f:\n",
        "  data = json.load(f)"
      ],
      "metadata": {
        "id": "2KvGjqbopjZY"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs = [LangChainDocument(page_content=e['text'], metadata={'topic': e['topic']}) for e in data]"
      ],
      "metadata": {
        "id": "DQ152ZAy84dN"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectordb = Chroma.from_documents(\n",
        "  documents=docs,\n",
        "  embedding=AzureOpenAIEmbeddings(model=\"text-embedding-ada-002\"),\n",
        "  persist_directory=\"/content/drive/MyDrive/data/chroma/\"\n",
        ")\n",
        "vectordb._collection.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O2JI_oy_8uXR",
        "outputId": "8c002a31-ec88-4808-c959-d906510260e0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "28"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"\""
      ],
      "metadata": {
        "id": "aDrnivYxntVh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Similarity metrics"
      ],
      "metadata": {
        "id": "w9v0lpEFnQNi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectordb = Chroma(persist_directory=\"/content/drive/MyDrive/data/chroma/\")\n",
        "embeddings = vectordb.get(include=[\"embeddings\"])['embeddings']"
      ],
      "metadata": {
        "id": "9MpVAJKcnTmH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cosine\n",
        "# l2"
      ],
      "metadata": {
        "id": "QUvhtizfnVqz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lexical search"
      ],
      "metadata": {
        "id": "ghBbLzjig9RS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### BM25"
      ],
      "metadata": {
        "id": "72R_vMB0l-PY"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XcFX2Q6chkOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Semantic search"
      ],
      "metadata": {
        "id": "G61rqG80hCD6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### LangChain similarity search"
      ],
      "metadata": {
        "id": "OmiHUt6Jl0-B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectordb = Chroma(persist_directory=\"/content/drive/MyDrive/data/chroma/\")"
      ],
      "metadata": {
        "id": "kbupUcGfaZiS"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# embedding_model = AzureOpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
        "# embedding_model.embed_query(\"Hello\")"
      ],
      "metadata": {
        "id": "q4zZnGHkhIk1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents = vectordb.similarity_search_with_score(query=\"Hello!\", k=10)"
      ],
      "metadata": {
        "id": "spccrV_RdvRB"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### HuggingFace Bi-Encoder"
      ],
      "metadata": {
        "id": "gZexftYamDDq"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vBoa17pOmSg3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reranking"
      ],
      "metadata": {
        "id": "hA-uqGVihQnw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### HuggingFace Cross-Encoder"
      ],
      "metadata": {
        "id": "ElDUjYQxkW_e"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i1H-CoAxlFb2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### OpenAI Completions as Cross-Encoder"
      ],
      "metadata": {
        "id": "xChzhCPrk7Ok"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CvtWjM2dhSA-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Two-stage retrieval"
      ],
      "metadata": {
        "id": "4dZZumIzltXY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# langchain compression"
      ],
      "metadata": {
        "id": "FiQBro63lvJr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
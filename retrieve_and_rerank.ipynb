{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNDeHSCFFtSoZukt9gBPqa9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mertcan-basut/nlp/blob/main/retrieve_and_rerank.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Background Information"
      ],
      "metadata": {
        "id": "N1YVOZkMsSs3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Context recall\n",
        "\n",
        "**recall** *(retrieval evaluation metric)* : How many of the relevant documents are retrieved.\n",
        "\n",
        "`recall@K= # of relevant docs returned / # of relevant documents in dataset`\n",
        "\n",
        "### LLM recall\n",
        "\n",
        "![LLM recall](https://www.pinecone.io/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2Fca206b6ada9163bffad313e0e18feee0b460c768-1212x688.png&w=1920&q=75)\n",
        "\n",
        "**LLM recall** refers to the ability of an LLM to find information from the text placed within its context window.\n",
        "\n",
        "When storing information in the middle of a context window, an LLM's ability to recall that information becomes worse than had it not been provided in the first place.\n",
        "\n",
        "### Two-stage retrieval\n",
        "\n",
        "A **reranking model (cross-encoder)** is a type of model that, given a query and document pair, will output a similarity score. Rerankers are much more accurate than embedding models (bi-encoder). But they are slow, so that is why two-stage retrieval is required to perform reranking on a small set of documents retrieved from a large set.\n",
        "\n",
        "![reranker/cross-encoder](https://www.pinecone.io/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F9f0d2f75571bb58eecf2520a23d300a5fc5b1e2c-2440x1100.png&w=3840&q=75)\n",
        "\n",
        "A reranker can receive the raw information directly into the large transformer computation, meaning less information loss. Rerankers run at user query time, and this allows analyzing the document's meaning specific to the user query.\n",
        "\n",
        "![embedding model/bi-encoder](https://www.pinecone.io/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F4509817116ab72e27bae809c38cb48fbf1578b5d-2760x1420.png&w=3840&q=75)\n",
        "\n",
        "Bi-encoders must compress all of the possible meanings of a document into a single vector resulting in information loss. Additionally, bi-encoders have no context on the query because the embeddings are created before user query time.\n",
        "\n",
        "### Sources\n",
        "üåêhttps://www.pinecone.io/learn/series/rag/rerankers/"
      ],
      "metadata": {
        "id": "dueV0d6Zimhl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementation"
      ],
      "metadata": {
        "id": "vymIHTEuskdi"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2KvGjqbopjZY"
      },
      "execution_count": 1,
      "outputs": []
    }
  ]
}